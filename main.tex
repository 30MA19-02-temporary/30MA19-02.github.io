% !TeX root = main.tex
%!TeX spellcheck = en_US
%!TeX encoding = utf8
%!TeX program = pdflatex

% https://apastyle.apa.org/style-grammar-guidelines
% https://www.ams.org/publications/authors/AMS-StyleGuide-online.pdf
\documentclass[stu, babel, american, biblatex, a4paper, leqno, draftall]{apa7}
\usepackage{csquotes}
\addbibresource{lists/preliminary.bib}
\DeclareLanguageMapping{american}{american-apa}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{tensor}
\usepackage{braket}
\usepackage{hyperref}
\usepackage[capitalise, sort&compress, noabbrev, nameinlink]{cleveref}

\iffalse % memory exceeded, include precompiled .pdf instead.
\iffalse % standalone subpreambles invoke errors, manually import instead.
\usepackage[subpreambles=true]{standalone}
\else
\usepackage{standalone}

\usepackage{pgf-umlcd}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

% \usetikzlibrary{external}
% \tikzexternalize
\fi
\usepackage{import}
\newcommand{\insertstandalone}[2]{\import{#1}{#2}}
\else
\newcommand{\insertstandalone}[2]{\includegraphics[width=\textwidth]{#1/#2.pdf}}
\fi

\allowdisplaybreaks
\crefname{paragraph}{paragraph}{paragraphs}
\Crefname{paragraph}{Paragraph}{Paragraphs}
\crefname{subparagraph}{subparagraph}{subparagraphs}
\Crefname{subparagraph}{Subparagraph}{Subparagraphs}


\title{Prove of Constant Radius Model}
\shorttitle{30MA19-02}
\leftheader{Hanchai, Sakepisit}
\authorsnames{Hanchai Nonprasart, Sakepisit Maysamat}
\authorsaffiliations{Mahidol Wittayanusorn School}
\course{SCI 30196: Science Project 1}
\professor{Amornsri Amornvatcharapong, Teerapong Suksumran}
\duedate{\today} % Last edit day when the draft watermark is removed.
\abstract{This is the first generation of the model. It's a draft. Who would even care to read the abstract when it's not done anyways?}
\keywords{None 1, None 2, None 3}

\input{lists/notations.tex}
\input{lists/theories.tex}

\begin{document}
\maketitle
\tableofcontents

\section{Rewrite note}
This proof will be organized using the following guideline
\begin{APAenumerate}
    \item reviewing existing definitions and theorem
    \item proving certain property of the existing mathematical objects
    \item defining the conditions of the model
    \item formulating of the model
    \item parameterize the model
    \item asserting defined property of the model
\end{APAenumerate}
It turned out that figures will crash overleaf, if you want to view such file, please compile them seperately.
To merge the file, it is needed to either pay the subscription or download and run it on your computer.

\section{Preliminary}

\subsection{Tensor}

\begin{equation}\label{Tensor:Sum}
    \tensor{\left(A+B\right)}{
        ^{i_1}^{\dots}^{i_n}
            _{j_1}_{\dots}_{j_m}
    }
    =
    \tensor{A}{
        ^{i_1}^{\dots}^{i_n}
            _{j_1}_{\dots}_{j_m}
    }
    +
    \tensor{B}{
        ^{i_1}^{\dots}^{i_n}
            _{j_1}_{\dots}_{j_m}
    }
\end{equation}
\begin{equation}\label{Tensor:Product:Scalar}
    \tensor{\left(\alpha A\right)}{
        ^{i_1}^{\dots}^{i_n}
            _{j_1}_{\dots}_{j_m}
    }
    =
    \alpha
    \tensor{A}{
        ^{i_1}^{\dots}^{i_n}
            _{j_1}_{\dots}_{j_m}
    }
\end{equation}
\begin{equation}\label{Tensor:Product:Tensor}
    \tensor{\left(A\otimes B\right)}{
        ^{i_1}^{\dots}^{i_l}^{i_{l+1}}^{\dots}^{i_{l+n}}
            _{j_1}_{\dots}_{j_k}_{j_{k+1}}_{\dots}_{j_{k+m}}
    }
    =
    \tensor{A}{
        ^{i_1}^{\dots}^{i_l}
            _{j_1}_{\dots}_{j_k}
    }
    \tensor{B}{
        ^{i_{l+1}}^{\dots}^{i_{l+n}}
            _{j_{k+1}}_{\dots}_{j_{k+m}}
    }
\end{equation}
\begin{equation}\label{Tensor:Contraction}
    \tensor{\left(\contr{\tensor{T}{}}\right)}{
        ^{i_1}^{\dots}^{i_n}
            _{j_1}_{\dots}_{j_m}
    }
    =
    \sum_a{\tensor{T}{
    ^{i_1}^{\dots}^{i_n}^a
    _a_{j_1}_{\dots}_{j_m}
    }}
\end{equation}
\begin{equation}\label{Matrix:Product}
    \tensor{\left(AB\right)}{^i_j}
    =\contr{\left(A\otimes B\right)}
    =\sum_k\tensor{A}{^i_k}\tensor{B}{^k_j}
\end{equation}
\begin{equation}\label{Matrix:Product:Block}
    \left(\begin{bmatrix}
        A_{11} & A_{12} \\
        A_{21} & A_{22} \\
    \end{bmatrix}
    \begin{bmatrix}
        B_{11} & B_{12} \\
        B_{21} & B_{22} \\
    \end{bmatrix}\right)_{ij}
    =
    \sum_k A_{ik} B_{kj}
\end{equation}
\begin{equation}\label{Matrix:Identity}
    IA = A = AI
\end{equation}
\begin{equation}\label{Matrix:Identity:Value}
    I_n=\diag{\left(1,1,\dots,1\right)}
\end{equation}
\begin{equation}\label{Matrix:Identity:Block}
    I_{a+b}=
    \begin{bmatrix}
        I_a           & 0_{a\times b} \\
        0_{b\times a} & I_b           \\
    \end{bmatrix}
\end{equation}
\begin{equation}\label{Matrix:Permutation:Square}
    T_{a,b} T_{a,b}= I
\end{equation}

\subsection{Differential}
\begin{equation}
    J_xF = \left(\left.\frac{\partial F_i}{\partial x_j}\right|_x\right)_{i,j}
\end{equation}
\begin{equation}
    \left.Df\right|_x
    : T_x\R^k\to T_{f(x)}\R^n
    = \left(x,v\right) \mapsto \left(f(x),J_xf(v)\right)
\end{equation}

\subsection{Group}

\begin{definition}[Lie group {\autocite[][Chapter~7]{lee_2013}}]\label{Group:Lie}
    A \textit{Lie group} is a smooth manifold $G$ (without boundary)
    that is also a group in the algebraic sense,
    with the property that
    the multiplication map $m:G\times G\to G$
    and the inversion map $i:G\to G$, given by
    \begin{equation*}
        m\left(g,h\right)=gh\text{,}
        \quad\quad
        i\left(g\right)=g^{-1}
    \end{equation*}
    are both smooth.
\end{definition}

\begin{proposition}[Lie group {\autocite[][Chapter~7]{lee_2013}}]\label{Group:Lie:Assertion}
    If $G$ is a smooth manifold with a group structure such that the map $G\times G\to G$ given by $\left(g,h\right)\mapsto gh^{-1}$ is smooth, then $G$ is a Lie group.
\end{proposition}

\begin{definition}[Semidirect product]\label{Group:SemidirectProduct}
    Suppose $H$ and $N$ are groups,
    and $\theta:H\times N\to N$ is a smooth left action of $H$ on $N$.
    It is said to be an \textit{action by automorphisms}
    if for each $h\in H$, the map $\theta_h:N\to N$ is a group automorphism of $N$ (i.e., an isomorphism from $N$ to itself).
    Given such action, we define a new group $N\rtimes_\theta H$,
    called a \textit{semidirect product} of $H$ and $N$, as follows.
    $N\rtimes_\theta H$ is just the Cartesian product $N\times H$;
    but the group multiplication is defined by
    $$\left(n,h\right)\left(n^\prime,h^\prime\right)=\left(n\theta_h\left(n^\prime\right),hh^\prime\right)\text{.}$$
\end{definition}

\begin{definition}\label{OrthogonalGroup}
    An orthogonal group $O\left(n\right)$ is a group of orthogonal matrix $M$ where $M^{-1}=M^T$ with matrix multiplication.
\end{definition}
\begin{definition}\label{SpecialOrthogonalGroup}
    An special orthogonal group $SO\left(n\right)$ is a subgroup of the orthogonal group $O\left(n\right)$ whose element $M$ have the property that $\det{M}=1$.
\end{definition}
\begin{definition}\label{IndefiniteOrthogonalGroup}
    An indefinite orthogonal group $O\left(m,n\right)$ is a group of indefinite orthogonal matrix $M$ where $M^{-1}=gM^Tg$ for $g=\diag{\left(-1,\dots,-1,1,\dots,1\right)}$ with matrix multiplication.
\end{definition}
\begin{definition}\label{OrthochronusIndefiniteOrthogonalGroup}
    An orthochronus indefinite orthogonal group $O^+\left(m,n\right)$ is a subgroup of the indefinite orthogonal group $O\left(m,n\right)$ whose element $M$ have the property that $\tensor{M}{^1_1}>0$.
\end{definition}
\begin{definition}\label{SpecialOrthochronusIndefiniteOrthogonalGroup}
    An special orthochronus indefinite orthogonal group $SO^+\left(m,n\right)$ is a subgroup of the orthochronus indefinite orthogonal group $O^+\left(m,n\right)$ whose element $M$ have the property that $\det{M}=1$.
\end{definition}
\begin{definition}\label{TranslationGroup}
    An translation group $T\left(n\right)$ is a group of vectors in $\R^n$ with vector addition.
\end{definition}
\begin{definition}\label{EuclideanGroup}
    An euclidean group $E\left(n\right)$ is a semidirect product of orthogonal group $O\left(n\right)$ extended by translation group $T\left(n\right)$.
\end{definition}

\begin{definition}\label{KleinGeometry}
    A \textit{Klein geometry} is a pair $\left(G, H\right)$
    where $G$ is a Lie group
    and $H$ is a closed Lie subgroup of $G$
    such that the (left) coset space $$X\defeq G / H$$ is connected.
\end{definition}
\begin{example}\label{KleinGeometryExample}
    KleinGeometryExamples (I)
\end{example}

\subsection{Manifold}

\begin{definition}[Abstract differentiable manifold {\autocite[][Chapter~5A]{kuhnelwolfgang_2006}}]\label{Manifold}
    A \textit{$k$-dimensional differentiable manifold} (briefly: a $k$-manifold)
    is a set $M$ together with a family $\left(M_i\right)_{i\in I}$ of subsets such that
    \begin{APAenumerate}
        \item $M=\bigcup_{i\in I} M_i$ (union),
        \item for every $i\in I$ there is an injective map $\varphi_i:M_i\to\R^k$ so that $\phi_i\left(M_i\right)$ is open in $\R^k$, and
        \item for $M_i\cap M_j\ne\emptyset$, $\varphi_i\left(M_i\cap M_j\right)$ is open in $\R^k$.
    \end{APAenumerate}
\end{definition}

\begin{definition}[Structures on a manifold {\autocite[][Chapter~5A]{kuhnelwolfgang_2006}}]\label{Manifold:Extended}
    Given a $k$-dimensional differentiable manifold,
    one gets additional structure
    by replacing aditional requirements on the transformation functions $\varphi_j\circ\varphi_i^{-1}$,
    which belong to the atlas of the manifold;
    if all $\varphi_j\circ\varphi_i^{-1}$ are (left-hand side),
    then one speaks of (right-hand side) as follows:
    \begin{center}
        \begin{tabular}{ r c l }
            continuous                & $\leftrightarrow$ & topological manifold                                 \\
            differentiable            & $\leftrightarrow$ & differentiable manifold                              \\
            $C^1$-differentiable      & $\leftrightarrow$ & $C^1$-manifold                                       \\
            $C^r$-differentiable      & $\leftrightarrow$ & $C^r$-manifold                                       \\
            $C^\infty$-differentiable & $\leftrightarrow$ & $C^\infty$-manifold                                  \\
            real analytic             & $\leftrightarrow$ & real analytic manifold                               \\
            complex analytic          & $\leftrightarrow$ & complex analytic manifold of dimension $\frac{k}{2}$ \\
            affine                    & $\leftrightarrow$ & affine manifold                                      \\
            projective                & $\leftrightarrow$ & projective manifold                                  \\
            conformal                 & $\leftrightarrow$ & manifold with a conformal structure                  \\
            orienatation-preserving   & $\leftrightarrow$ & orientable manifold                                  \\
        \end{tabular}
    \end{center}
\end{definition}

\begin{definition}[Tangent vector {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:TangentVector}
    A \textit{tangent vector} $X$ at $p$
    is a derivation (derivative operator) defined on the set of \textit{germs of functions}
    $$\mathcal{F}_p\left(M\right)\defeq\set{f:M\to\R|f\text{ differentiable}}/\sim\text{,}$$
    where the equivalence relation $\sim$ is defined by
    declaring $f\sim f^\ast$ if and only if
    $f$ and $f^\ast$ coincide in a neighborhood of $p$.
    The value $X\left(f\right)$ is also referred to as the \textit{directional derivative} of $f$ in the direction $X$.

    This definition means more precisely the following.
    $X$ is a map $X:\mathcal{F}_p\left(M\right)\to\R$
    with the two following properties:
    \begin{APAenumerate}
        \item $X\left(\alpha f+\beta g\right)=\alpha X\left(f\right)+\beta\left(g\right)$, $f,g\in\mathcal{F}_p\left(M\right)$ (\textit{$\R$-linearity});
        \item $X\left(f\cdot g\right)=X\left(f\right)\cdot g\left(p\right)+f\left(p\right)\cdot X\left(g\right)$ for $f,g\in\mathcal{F}_p\left(M\right)$ (\textit{product rule}).
    \end{APAenumerate}
    (For this to make sense, both $f$ and $g$ have to be defined in a neighborhood of $p$.)

    Briefly: \textit{tangent vectors are derivations acting on scalar functions.}
\end{definition}

\begin{corollary}[Tangent space {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:TangentSpace}
    The \textit{tangent space} $T_pM$ of $M$ at $p$
    is defined in all cases as
    the set of all tangent vectors at the point $p$.
    By definition $T_pM$ and $T_qM$ are disjoint if $p\ne q$.
\end{corollary}

\begin{definition}[Derivative {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:Derivative}
    Let $F:M\to N$ be a differentiable map,
    and let $p$, $q$ be two fixed point with $F\left(p\right)=q$.
    Then the \textit{derivative} or the $\textit{differential}$ of $F$ at $p$ is defined as the map
    $$\left.DF\right|_p:T_pM\to T_qN$$
    whose value at $X\in T_pM$ is given by
    $\left(\left.DF\right|_p\left(X\right)\right)\left(f\right)=X\left(f\circ F\right)$
    for every $f\in \mathcal{F}_q\left(N\right)$
    (which automatically implies the relation $f\circ F\in\mathcal{F}_p\left(M\right)$).
\end{definition}

\begin{lemma}[Chain rule {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:ChainRule}
    For the derivative as defined in this manner, one has the \textit{chain rule} in the form
    $$\left.D\left(G\circ F\right)\right|_p=\left.DG\right|_{F\left(p\right)}\circ\left.DF\right|_p$$
    for every composition $M\xrightarrow{F}N\xrightarrow{G}Q$ of maps, or, more briefly, $D\left(G\circ F\right)=DG\circ DF$.
\end{lemma}

\begin{definition}[Riemannian metric {\autocite[][Chapter~5C]{kuhnelwolfgang_2006}}]\label{Manifold:RiemannianMetric}
    A \textit{Riemannian metric} $g$ on $M$
    is an association $p\mapsto g_p\in L^2\left(T_pM;\R\right)$
    such that the following conditins are satisfied:
    \begin{APAenumerate}
        \item $g_p\left(X,Y\right)=g_p\left(Y,X\right)$ for all $X$, $Y$, \hfill (\textit{symmetry})
        \item $g_p\left(X,X\right)>0$ for all $X\ne0$, \hfill (\textit{positive definiteness})
        \item The coefficient $\tensor{g}{_i_j}$ in every local representation (i.e., in every chart) $$g_p=\sum_{i,j}\tensor{g}{_i_j}\left(p\right)\cdot \left.d\tensor{x}{^i}\right|_p \otimes\left.d\tensor{x}{^j}\right|_p$$ are differentiable functions. \hfill (\textit{differentiability})
    \end{APAenumerate}
\end{definition}

\begin{remark}\label{Manifold:Riemannian}
    The pair $\left(M,g\right)$ is then called \textit{Riemannian manifold}.
    One also refers to the Riemannian metric as the \textit{metric tensor}.
    In local coordinates the metric tensor is given by the matrix ($\tensor{g}{_i_j}$) of functions.
    In Ricci calculus this is simply written as $\tensor{g}{_i_j}$.
        {\autocite[][Chapter~5C]{kuhnelwolfgang_2006}}
\end{remark}

\begin{remark}\label{Manifold:InnerProduct}
    A Riemannian metric $g$ defines at every point $p$
    an \textit{inner product} $g_p$ on the tangent space $T_pM$,
    and therefore the notation $\inner{X}{Y}$ instead of $g_p\left(X,Y\right)$ is also used.
    The notions of angles and lengths are determined by this inner product,
    just as these notions are determined by the first fundamental form on surface elements.
    The length or norm of vector $X$ is given by $\norm{X}\defeq\sqrt{g\left(X,X\right)}$,
    and the angle $\beta$ between two tangent vectors $X$ and $Y$
    can be defined by the validity of the equation $\cos\beta\cdot\norm{X}\cdot\norm{Y}=g\left(X,Y\right)$.
        {\autocite[][Chapter~5C]{kuhnelwolfgang_2006}}
\end{remark}

\subsection{Curvature}
\begin{definition}[Christoffel symbols {\autocite[][Chapter~4A]{kuhnelwolfgang_2006}}]\label{Manifold:ChristoffelSymbol}
    
    \begin{APAenumerate}
        \item The quantities $\partial_k\tensor{\Gamma}{_i_j}$ defined by the expressions
              \begin{equation*}
                \partial_k\tensor{\Gamma}{_i_j}
                \defeq
                \inner{\nabla_{\frac{\partial f}{\partial\tensor{u}{^i}}}\frac{\partial f}{\partial\tensor{u}{^j}}}{\frac{\partial f}{\partial\tensor{u}{^k}}}
              \end{equation*}
              are called the \textit{Christoffel symbols of the first kind}.
        \item The quantities $\tensor{\Gamma}{^k_i_j}$ defined by the expressions
        \begin{equation*}
          \nabla_{\frac{\partial f}{\partial\tensor{u}{^i}}}\frac{\partial f}{\partial\tensor{u}{^j}}
          =
          \sum_k{\tensor{\Gamma}{^k_i_j}\frac{\partial f}{\partial\tensor{u}{^k}}}
        \end{equation*}
        are called the \textit{Christoffel symbols of the second kind}.
        \item By definition one has $\partial_k\tensor{\Gamma}{_i_j}=\partial_k\tensor{\Gamma}{_j_i}$, $\tensor{\Gamma}{^k_i_j}=\tensor{\Gamma}{^k_j_i}$
        as well as $\partial_k\tensor{\Gamma}{_i_j}=\sum_{m}\tensor{\Gamma}{^m_i_j}\tensor{g}{_m_k}$.
    \end{APAenumerate}
\end{definition}
\begin{definition}[Gauss map {\autocite[][Chapter~3B]{kuhnelwolfgang_2006}}]\label{Curvature:Gauss}
    For a surface element $f:U\to\R^{3}$, the \textit{Gauss map}
    \begin{equation*}
        \nu:U\to S^2
    \end{equation*}
    is defined by the formula
    \begin{equation*}
        \nu(u_1, u_2)
        \defeq
        \frac
        {\frac{\partial f}{\partial u_1}\times\frac{\partial f}{\partial u_2}}
        {\norm{\frac{\partial f}{\partial u_1}\times\frac{\partial f}{\partial u_2}}}
        \text{.}
    \end{equation*}
\end{definition}
\begin{definition}[Weingarten map, shape operator {\autocite[][Chapter~3B]{kuhnelwolfgang_2006}}]\label{Curvature:ShapeOperator}
    Let $f:U\to\R^{3}$ be a surface element with Gauss map $\nu:U\to S^2\subset\R^3$.
    \begin{APAenumerate}
        \item For every $u\in U$ the image plane of the bilinear map
        \begin{equation*}
            \left.D\nu\right|_u:T_uU\to T_{\nu(u)}\R^3
        \end{equation*}
        is parallel to the tangent plane $T_uf$.
        By canonically identifying $T_{\nu(u)}\R^3\cong\R^3\cong T_{f(u)}\R^3$
        we may therefore view $D\nu$ at every point as the map
        \begin{equation*}
            \left.D\nu\right|_u:T_uU\to T_uf \text{.}
        \end{equation*}
        Moreover, by restricting to the image,
        we may view the map $\left.Df\right|)u$ as a linear isomorphism
        \begin{equation*}
            \left.Df\right|_u:T_uU\to T_uf \text{.}
        \end{equation*}
        In this sense
        the inverse mapping $\left(\left.Df\right|_u\right)^{-1}$ is well-defined
        and is also an isomorphism
        \item The map $L\defeq -D\nu\circ\left(Df\right)^{-1}$ defined pointwise by
        \begin{equation*}
            L_u
            \defeq -\left(\left.D\nu\right|_u\right)\circ\left(\left.Df\right|_u\right)^{-1}
            : T_uf\to T_uf
        \end{equation*}
        is called the \textit{Weingarten map} or the \textit{shape operator} of $f$.
        Obviously, for every parameter $u$ this is a linear endomorphism of the tangent plane at the corresponding point $f(u)$.
        \item $L$ is independent of the parameterization $f$ (up to the choice of the sign of the unit normal vector $\nu$),
        and it is self-adjoint with respect to the first fundamental form $I$.
    \end{APAenumerate}
\end{definition}
\begin{definition}[Hypersurface element {\autocite[][Chapter~3F]{kuhnelwolfgang_2006}}]\label{Manifold:Hypersurface}
    $f:U\to\R^{n+1}$ is called a \textit{regular hypersurface element},
    if $U\subset\R^n$ is open and $f$ is a ($C^2-$) immersion.
    The parameter $u=\left(u_1,\dots,u_n\right)$ is associated with the point $f(u)$
    with $n+1$ coordinates $f(u)=\left(f_1(u),\dots,f_{n+1}(u)\right)$.
    The \textit{tangent hyperplane} $T_uf$ is the is defined to be the image of $T_uU$ under the map $\left.Df\right|_u$.
    Similarly, one defines
    \begin{APAitemize}
        \item the \textit{Gauss map} $\nu:U\to S^n$ by the unit normal vector $\nu(u)$,
        which is perpendicular to $T_uf$
        (
            but note: in $\R^{n+1}$ for $n\ge3$ there is no bilinear vector product of tangent vectors;
            still one can formally define $\nu$ as an $n$-linear vector product
        ),
        \item the \textit{Weingarten map} $L=-D\nu\circ\left(Df\right)^{-1}$,
        \item the \text{first, second, and third fundamental forms}
        \begin{align*}
            I &= \left(\tensor{g}{_i_j}\right)_{i,j=1,\dots,n} &= \left(\inner{\frac{\partial f}{\partial u_i}}{\frac{\partial f}{\partial u_j}}\right)\text{,} \\
            II &= \left(\tensor{h}{_i_j}\right)_{i,j=1,\dots,n} &= \left(\inner{\frac{\partial^2 f}{\partial u_i\partial u_j}}{\nu}\right)\text{,} \\
            III &= \left(\tensor{e}{_i_j}\right)_{i,j=1,\dots,n} &= \left(\inner{\frac{\partial \nu}{\partial u_i}}{\frac{\partial \nu}{\partial u_j}}\right)\text{.} \\
        \end{align*}
    \end{APAitemize}
\end{definition}
\begin{definition}[Curvature tensor {\autocite[][Chapter~4C]{kuhnelwolfgang_2006}}]\label{Manifold:CurvatureTensor}
    \begin{equation*}
        R\left(X,Y\right)Z
        \defeq
        \nabla_X\nabla_YZ - \nabla_Y\nabla_XZ - \nabla_{\lie{X}{Y}}Z
    \end{equation*}
    is a tensor field, which is called the \textit{curvature tensor} of the surface.
\end{definition}
\begin{remark}[Curvature tensor {\autocite[][Chapter~4C]{kuhnelwolfgang_2006}}]\label{Manifold:CurvatureTensor:ChristoffelSymbol}
    The left-hand side of the Gauss equation is called the \textit{curvature tensor}
    and is in general expressed in the form
    \begin{equation*}
        \tensor{R}{^s_i_k_j}
        \defeq
        \frac{\partial}{\partial\tensor{u}{^k}}\tensor{\Gamma}{^s_i_j}
        - \frac{\partial}{\partial\tensor{u}{^j}}\tensor{\Gamma}{^s_i_k}
        + \sum_{r}\left(\tensor{\Gamma}{^r_i_j}\tensor{\Gamma}{^s_r_k}-\tensor{\Gamma}{^r_i_k}\tensor{\Gamma}{^s_r_j}\right) \text{.}
    \end{equation*}
\end{remark}

\begin{example}\label{ShapeOperator}
    $$S\left(v\right)=\pm\nabla_v n$$
\end{example}
\begin{example}\label{PrincipalCurvature}
    Eigenvalue of second fundamental form (or shape operator)
\end{example}

\section{Objective}
\paragraph{Objective}\label{Objective}
The objective is that
given a natural number $n$ and a real number $\kappa$,
one can construct
\begin{APAenumerate}
    \item a set $M$
    \item a Lie group $M$ with operation $\otimes_M$,
    \item an $n$-dimensional $C^\infty$-manifold $M$ with chart $\varphi_i \subset M\to\R^n$,
    \item a Riemannian manifold $M$ with inner product $g_p\in T_pM\times T_pM\to\R$
\end{APAenumerate}
(to be determined)
such that
\begin{APAitemize}
    \item group action is distance preserved.
    \item model is continuous with respect to $\kappa$ (and smooth with respect to each basis).
    \item for all two-dimensional linear subspaces of the manifold, the sectional curvature is $\kappa$.
\end{APAitemize}
(cannot figure formal definition out yet.)

\begin{conjecture}\label{GeometricGroupStructure}
    If the parameters $\left(\kappa,n\right)$ is associated with Klein geometry $\left(G,H\right)$
    then $\left(M,\otimes_M\right)\cong G/H$.

    That is, from \cref{KleinGeometryExample},
    \begin{APAitemize}
        \item For $\kappa>0$, $G\cong O\left(n+1\right)$ and $H\cong O\left(n\right)$.
        \item For $\kappa=0$, $G\cong E\left(n\right)$ and $H\cong O\left(n\right)$.
        \item For $\kappa<0$, $G\cong O^{+}\left(1,n\right)$ and $H\cong O\left(n\right)$.
    \end{APAitemize}
\end{conjecture}

\section{Model Foundation}
\subsection{Trigonometry}
\begin{definition}\label{M:Trigonometry}
    \textit{Generalized trigonometric functions} $f_k:\R\to\R$ and $f_k^\ast:\R\to\R$ are defined as
    \begin{align*}
        f_k\left(\theta\right)      & \defeq
        \begin{cases}
            g\left(k\theta\right) & \text{if $k\geq0$,} \\
            h\left(k\theta\right) & \text{otherwise,}   \\
        \end{cases} \\
        f_k^\ast\left(\theta\right) & \defeq
        \begin{cases}
            g\left(k\theta\right)  & \text{if $k\geq0$,} \\
            h\left(-k\theta\right) & \text{otherwise,}   \\
        \end{cases}
    \end{align*}
    where $g$ (resp. $h$) are the associated trigonometric (resp. hyperbolic) function.
\end{definition}
\begin{example}[Generalized sine functions]\label{M:Trigonometry:Sine}
    \begin{align*}
        \sin_k{\theta}      & \defeq
        \begin{cases}
            \sin\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \sinh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}                               \\
        \sin_k^\ast{\theta} & \defeq
        \begin{cases}
            \sin\left(k\theta\right)   & \text{if $k\geq0$,} \\
            \sinh\left(-k\theta\right) & \text{otherwise.}   \\
        \end{cases}                              \\
                            & =
        \begin{cases}
            \sin\left(k\theta\right)   & \text{if $k\geq0$,} \\
            -\sinh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}                              \\
                            & = \begin{cases}
                                    \sin\left(\abs{k}\theta\right)  & \text{if $k\geq0$,} \\
                                    \sinh\left(\abs{k}\theta\right) & \text{otherwise.}   \\
                                \end{cases}
    \end{align*}
    (see \cref{TrigonometrySinePlotted})
\end{example}
\begin{example}[Generalized cosine functions]\label{M:Trigonometry:Cosine}
    \begin{align*}
        \cos_k{\theta}      & \defeq
        \begin{cases}
            \cos\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \cosh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}                         \\
        \cos_k^\ast{\theta} & \defeq
        \begin{cases}
            \cos\left(k\theta\right)   & \text{if $k\geq0$,} \\
            \cosh\left(-k\theta\right) & \text{otherwise.}   \\
        \end{cases}                        \\
                            & = \begin{cases}
                                    \cos\left(k\theta\right)  & \text{if $k\geq0$,} \\
                                    \cosh\left(k\theta\right) & \text{otherwise.}   \\
                                \end{cases} \\
                            & = \cos_k{\theta}
    \end{align*}
    (see \cref{TrigonometryCosinePlotted})
\end{example}
\begin{theorem}[Pythagorean's identity equivalence]\label{M:Trigonometry:Pythagorean}
    \begin{equation*}
        \cos_k^2{\theta} +\sign{k}\sin_k^2{\theta} = 1
    \end{equation*}
\end{theorem}
\begin{proof}[\proofof{M:Trigonometry:Pythagorean}]
    Proof by exhaustion.
\end{proof}
\begin{proposition}[Generalized trigonometric functions of sum of arguments]\label{M:Trigonometry:Sum}
    \begin{align*}
        \sin_k\left(\theta+\phi\right)
         & = \sin_k{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k{\phi}           \\
        \sin_k^\ast\left(\theta+\phi\right)
         & = \sin_k^\ast{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k^\ast{\phi} \\
        \cos_k\left(\theta+\phi\right)
         & = \cos_k{\theta}\cos_k{\phi}-\sign{k}\sin_k{\theta}\sin_k{\phi}   \\
         & = \cos_k{\theta}\cos_k{\phi}-\sin_k^\ast{\theta}\sin_k{\phi}      \\
         & = \cos_k{\theta}\cos_k{\phi}-\sin_k{\theta}\sin_k^\ast{\phi}
    \end{align*}
\end{proposition}
\begin{proof}[\proofof{M:Trigonometry:Sum}]
    Proof by exhaustion.
\end{proof}
\begin{proposition}[Derivative of generalized trigonometric functions]\label{M:Trigonometry:Derivative}
    \begin{align*}
        {\sin_k}^\prime{\theta}      & = k\cos_k{\theta}       \\
        {\sin_k^\ast}^\prime{\theta} & = \abs{k}\cos_k{\theta} \\
        {\cos_k}^\prime{\theta}      & = -k\sin_k^\ast{\theta}
    \end{align*}
\end{proposition}
\begin{proof}[\proofof{M:Trigonometry:Derivative}]
    Proof by exhaustion.
\end{proof}
\subsection{Matrices}
\begin{definition}\label{M:Rotation}
    \textit{Generalized rotation matrix} is defined as
    \begin{align*}
        R_k\left(\theta\right) & \defeq
        \begin{bmatrix}
            \cos_k{\theta} & -\sin_k^\ast{\theta} \\
            \sin_k{\theta} & \cos_k{\theta}       \\
        \end{bmatrix}\text{,}
    \end{align*}
    where $\theta\in\R$.
\end{definition}
\begin{corollary}[Generalized rotation matrix at zero]\label{M:Rotation:Identity}
    \begin{equation*}
        R_k\left(0\right)
        =
        I_2
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Rotation:Identity}]
    Obvious
\end{proof}
\begin{corollary}[Generalized rotation matrix of sum of arguments]\label{M:Rotation:Sum}
    \begin{equation*}
        R_k\left(\theta\right)R_k\left(\phi\right)=R_k\left(\theta+\phi\right)
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Rotation:Sum}]
    \begin{align*}
        R_k\left(\theta\right)R_k\left(\phi\right)
         & =\begin{bmatrix}
                \cos_k{\theta} & -\sin_k^\ast{\theta} \\
                \sin_k{\theta} & \cos_k{\theta}       \\
            \end{bmatrix}\begin{bmatrix}
                             \cos_k{\phi} & -\sin_k^\ast{\phi} \\
                             \sin_k{\phi} & \cos_k{\phi}       \\
                         \end{bmatrix}                                                       & \text{(\cref{M:Rotation})}              \\
         & =\begin{bmatrix}
                \cos_k{\theta}\cos_k{\phi}+\left(-\sin_k^\ast{\theta}\right)\sin_k{\phi} &
                \cos_k{\theta}\left(-\sin_k^\ast{\phi}\right)+\left(-\sin_k^\ast{\theta}\right)\cos_k{\phi} \\
                \sin_k{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k{\phi}                    &
                \sin_k{\theta}\left(-\sin_k^\ast{\phi}\right)+\cos_k{\theta}\cos_k{\phi}                    \\
            \end{bmatrix} & \text{(\cref{Matrix:Product})}                                \\
         & =\begin{bmatrix}
                \cos_k{\theta}\cos_k{\phi}-\sin_k^\ast{\theta}\sin_k{\phi} &
                -\left(\sin_k^\ast{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k^\ast{\phi}\right) \\
                \sin_k{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k{\phi}      &
                \cos_k{\theta}\cos_k{\phi}-\sin_k{\theta}\sin_k^\ast{\phi}                    \\
            \end{bmatrix}               & \text{(simplify)}                                              \\
         & =\begin{bmatrix}
                \cos_k{\theta+\phi} & -\sin_k^\ast{\theta+\phi} \\
                \sin_k{\theta+\phi} & \cos_k{\theta+\phi}       \\
            \end{bmatrix}                                             & \text{(\cref{M:Trigonometry:Sum})}                             \\
         & = R_k\left(\theta+\phi\right)                                                                  & \text{(\cref{M:Rotation})} \\
        R_k\left(\theta\right)R_k\left(\phi\right)
         & = R_k\left(\theta+\phi\right)                                                                  & \qedhere
    \end{align*}
\end{proof}
\begin{corollary}[Inverse of generalized rotation matrix]\label{M:Rotation:Inverse}
    \begin{equation*}
        R_k\left(\theta\right)^{-1} = R_k\left(-\theta\right)
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Rotation:Inverse}]
    \begin{align*}
        R_k\left(\theta\right)R_k\left(-\theta\right)
         & = R_k\left(0\right) & \text{(\cref{M:Rotation:Sum})}      \\
         & = I_2               & \text{(\cref{M:Rotation:Identity})} \\
        R_k\left(-\theta\right)R_k\left(\theta\right)
         & = R_k\left(0\right) & \text{(\cref{M:Rotation:Sum})}      \\
         & = I_2               & \text{(\cref{M:Rotation:Identity})}
    \end{align*}
    \begin{equation*}
        R_k\left(\theta\right)R_k\left(-\theta\right)
        = R_k\left(-\theta\right)R_k\left(\theta\right)
        = I_2
    \end{equation*}
    \begin{equation*}
        {R_k\left(\theta\right)}^{-1}
        =
        R_k\left(-\theta\right)
        \qedhere
    \end{equation*}
\end{proof}
\begin{definition}\label{M:Position}
    \textit{Position matrix} is defined recursively as
    \begin{align*}
        P_{k,n}\left(\left\{\tensor{\theta}{^1},\dots,\tensor{\theta}{^n}\right\}\right) & \defeq
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1} \text{,}                                                                                    \\
        P_{k,0}                                                                          & \defeq I_1\text{,}
    \end{align*}
    where $\theta=\left\{\tensor{\theta}{^i}\right\}\in\R^n$ for $i\in\range{1}{n}$.
\end{definition}
\begin{definition}\label{M:Position:Set}
    Let $P\left(n,k\right)$ be set of position matrices.
\end{definition}
\begin{corollary}[Position matrix at zero]\label{M:Position:Set:Identity}
    \begin{equation*}
        P_{k,n}
        \left(0_{n}\right)
        =
        I_{n+1}
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Position:Set:Identity}]
    Prove by mathematical induction on $n$,
    Let
    \begin{equation}\label{M:Position:Set:Identity:Proof:Induction}
        P_{k,n-1}\left(0_{n-1}\right)=I_n
    \end{equation}
    \begin{align*}
        P_{k, n}\left(0_{n}\right)
                   & =
        \begin{bmatrix}
            P_{k,n-1}\left(0_{n-1}\right) & 0_{n\times 1} \\
            0_{1\times n}                 & 1             \\
        \end{bmatrix}
        T_{2, n+1}
        \begin{bmatrix}
            R_{k}\left(0\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2, n+1} & \text{(\cref{M:Position})}                              \\
                   & =
        \begin{bmatrix}
            I_{n}         & 0_{n\times 1} \\
            0_{1\times n} & 1             \\
        \end{bmatrix}
        T_{2, n+1}
        \begin{bmatrix}
            R_{k}\left(0\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2, n+1} & \text{(\cref{M:Position:Set:Identity:Proof:Induction})} \\
                   & =
        \begin{bmatrix}
            I_{n}         & 0_{n\times 1} \\
            0_{1\times n} & 1             \\
        \end{bmatrix}
        T_{2, n+1}
        \begin{bmatrix}
            I_{2}              & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}} & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2, n+1} & \text{(\cref{M:Rotation:Identity})}                     \\
                   & =
        I_{n+1}
        T_{2, n+1}
        I_{n+1}
        T_{2, n+1} & \text{(\cref{Matrix:Identity:Block})}                   \\
                   & =
        T_{2, n+1}
        T_{2, n+1} & \text{(\cref{Matrix:Identity})}                         \\
                   & =
        I_{n+1}    & \text{(\cref{Matrix:Permutation:Square})}
    \end{align*}
    \begin{equation*}
        P_{k,n}
        \left(0_{n}\right)
        =
        I_{n+1}
        \qedhere
    \end{equation*}
\end{proof}
\begin{definition}\label{M:Orientation}
    \textit{Orientation matrix} is defined as
    \begin{align*}
        Q^\pm_n\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) & \defeq
        \begin{bmatrix}
            1             & 0_{1\times n}                                                 \\
            0_{n\times 1} & X^\pm_{+1,n-1}\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) \\
        \end{bmatrix}\text{,}   \\
        Q^\pm_0                                                & \defeq \pm I_1\text{,}
    \end{align*}
    where $\phi_m\in\R^m \text{for } m\in\range{1}{n-1}$.
\end{definition}
\begin{definition}\label{M:Orientation:Set}
    Let $Q\left(n\right)$ be set of orientation matrices.
\end{definition}
\begin{corollary}[Orientation matrix at zero]\label{M:Orientation:Set:Identity}
    \begin{equation*}
        Q^{+}_{n}
        \left(0_{n-1}, 0_{n-2}, \dots\right)
        =
        I_{n+1}
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Orientation:Set:Identity}]
    Prove by mathematical induction on $n$,
    Let
    \begin{equation}\label{M:Orientation:Set:Identity:Proof:Induction}
        Q^+_{n-1}\left(0_{n-2}, 0_{n-3}, \dots\right)=I_n
    \end{equation}
    \begin{align*}
        Q^+_n\left(0_{n-1}, 0_{n-2}, \dots\right)
                                                                                                       & =
        \begin{bmatrix}
            1             & 0_{1\times n}                                    \\
            0_{n\times 1} & X^\pm_{+1,n-1}\left(0_{n-1},0_{n-2},\dots\right) \\
        \end{bmatrix}                            & \text{(\cref{M:Orientation})}                                                               \\
                                                                                                       & =
        \begin{bmatrix}
            1             & 0_{1\times n}                                                               \\
            0_{n\times 1} & P_{+1,n-1}\left(0_{n-1}\right)Q^+_{n-1}\left(0_{n-2}, 0_{n-3}, \dots\right) \\
        \end{bmatrix} & \text{(\cref{M:Point})}                                            \\
                                                                                                       & =
        \begin{bmatrix}
            1             & 0_{1\times n}                      \\
            0_{n\times 1} & P_{+1,n-1}\left(0_{n-1}\right) I_n \\
        \end{bmatrix}                                          & \text{(\cref{M:Orientation:Set:Identity:Proof:Induction})}                    \\
                                                                                                       & =
        \begin{bmatrix}
            1             & 0_{1\times n}                  \\
            0_{n\times 1} & P_{+1,n-1}\left(0_{n-1}\right) \\
        \end{bmatrix}                                              & \text{(\cref{Matrix:Identity})}                                           \\
                                                                                                       & =
        \begin{bmatrix}
            1             & 0_{1\times n} \\
            0_{n\times 1} & I_{n}         \\
        \end{bmatrix}                                                               & \text{(\cref{M:Position:Set:Identity})}                  \\
                                                                                                       & =
        I_{n+1}                                                                                        & \text{(\cref{Matrix:Identity:Block})}
    \end{align*}
    \begin{equation*}
        Q^+_n
        \left(0_{n-1}, 0_{n-2}, \dots\right)
        =
        I_{n+1}
        \qedhere
    \end{equation*}
\end{proof}
\begin{definition}\label{M:Point}
    \textit{Point matrix} is defined as
    \begin{align*}
        X^\pm_{k,n}\left(\theta,\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) & \defeq
        P_{k,n}\left(\theta\right)
        Q^\pm_n\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right)\text{,}
    \end{align*}
    where $\theta\in\R^n$ and $\phi_m\in\R^m \text{for } m\in\range{1}{n-1}$.
\end{definition}
\begin{definition}\label{M:Point:Set}
    Let $X\left(n,k\right)$ be set of point matrices.
\end{definition}
\begin{corollary}[Position matrix as subset of point matrix]\label{M:Point:Position}
    \begin{equation*}
        P\left(k,n\right)\subset X\left(k,n\right)
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Point:Position}]
    \begin{align*}
        \forall{P \in P\left(k,n\right)}
        \forall{Q\in Q\left(n\right)},
                                  &
        P Q \in X\left(k,n\right) & \text{(\cref{M:Point})}                    \\
        \implies                  &
        P I \in X\left(k,n\right) & \text{(\cref{M:Orientation:Set:Identity})} \\
        \implies                  &
        P \in X\left(k,n\right)   & \text{(\cref{Matrix:Identity})}
    \end{align*}
    \begin{equation*}
        P\left(k,n\right)\subset X\left(k,n\right) \qedhere
    \end{equation*}
\end{proof}
\begin{corollary}[Point matrix at zero]\label{M:Point:Set:Identity}
    \begin{equation*}
        X^{+}_{k,n}
        \left(0_{n}, 0_{n-1}, \dots\right)
        =
        I_{n+1}
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Point:Set:Identity}]
    It can be implied from \cref{M:Position:Set:Identity,M:Orientation:Set:Identity}.
\end{proof}
\subsection{Group structure}
\begin{proposition}
    Group of position matrix with multiplication is a subgroup of an orthogonal group for $k>0$.
    \begin{equation*}
        \left(P\left(n, k\right),\cdot\right)\cong O\left(n+1\right)
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{align*}
        P&=P_{k,n}\left(\left\{\tensor{\theta}{^1},\dots,\tensor{\theta}{^n}\right\}\right) \\
        &\defeq
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1} && \text{(\cref{M:Position})} \\
        P^T
        &=
        T_{2,n+1}^T
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}^T
        T_{2,n+1}^T
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}^T && \text{(Transpose of product)} \\
        &=
        T_{2,n+1}^T
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right)^T & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1}^T
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{(Transpose of block)} \\
        &=
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right)^T & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{(Transpose of permutation matrix)} \\
        PP^T
        &=
        \left(\begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1}\right)\\
        &\left(T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right)^T & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}\right) && \text{(Equation above)} \\
        &=
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}\\
        &
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right)^T & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{()} \\
        &=
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right)R_k\left(\tensor{\theta}{^n}\right)^T & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}\\
        &
        T_{2,n+1}
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{(\cref{Matrix:Product:Block})} \\
        &=
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}\\
        &
        T_{2,n+1}
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{()} \\
        &=
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{()} \\
        &=
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{(\cref{Matrix:Product:Block})} \\
        &= I_{n+1} && \text{(mathematical induction)}
    \end{align*}
\end{proof}
\begin{proposition}
    Group of position matrix with multiplication is a subgroup of an $\left(1,n\right)$-orthochronus indefinite orthogonal group for $k<0$.
    \begin{equation*}
        \left(P\left(n, k\right),\cdot\right)\cong O^+\left(1,n\right)
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{align*}
        P&=P_{k,n}\left(\left\{\tensor{\theta}{^1},\dots,\tensor{\theta}{^n}\right\}\right) \\
        &\defeq
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1} && \text{(\cref{M:Position})} \\
        P^T
        &=
        T_{2,n+1}^T
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}^T
        T_{2,n+1}^T
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}^T && \text{(Transpose of product)} \\
        &=
        T_{2,n+1}^T
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right)^T & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1}^T
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{(Transpose of block)} \\
        &=
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right)^T & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right)^T & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix} && \text{(Transpose of permutation matrix)} \\
        g
        &= \diag{-1,1,\dots,1} \\
        gPgP^T
        &= I_{n+1} && \text{(mathematical induction)}
    \end{align*}
    \begin{align*}
        \tensor{P}{^1_1} &> 0
    \end{align*}
\end{proof}
\begin{proposition}
    Group of position matrix with multiplication is isomorphic to translation group for $k\to0$.
    \begin{equation*}
        \left(P\left(n, k\right),\cdot\right)\cong E\left(n\right)
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{align*}
        R_k\left(\theta\right)
        &\defeq\begin{bmatrix}
            \cos_k\left(\theta\right) & -\sin_k^\ast\left(\theta\right) \\
            \sin_k\left(\theta\right) & \cos_k\left(\theta\right) \\
        \end{bmatrix} && \text{(\cref{M:Rotation})} \\
        &\to\begin{bmatrix}
            1 & 0 \\
            k\theta & 1 \\
        \end{bmatrix} && \text{(Limits of the functions)} \\
        P_{k,n}\left(\left\{\tensor{\theta}{^1},\dots,\tensor{\theta}{^n}\right\}\right)
        &\defeq
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1} && \text{(\cref{M:Position})} \\
        &\to
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            \begin{matrix*}1&0\\k\tensor{\theta}{^n}&0\\\end{matrix*} & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                  & {I}_{n-1}          \\
        \end{bmatrix}
        T_{2,n+1} && \text{(Equation above)} \\
        &=
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                              & 1             \\
        \end{bmatrix}
        \begin{bmatrix}
            {I}_{n} & 0_{n \times1}          \\
            \begin{matrix*}k\tensor{\theta}{^n} & 0_{1\times n-1}\end{matrix*} & 1 \\
        \end{bmatrix} && \text{(property of permutation matrix)} \\
        &=
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n \times1}          \\
            \begin{matrix*}k\tensor{\theta}{^n} & 0_{1\times n-1}\end{matrix*} & 1 \\
        \end{bmatrix} && \text{(\cref{Matrix:Product:Block})} \\
        P_{k,n}\left(\tensor{\theta}{}\right)
        &\to
        \begin{bmatrix}
            1 & 0_{n \times1}          \\
            k\tensor{\theta}{} & I_n \\
        \end{bmatrix} && \text{(mathematical induction)} \\
        P_{k,n}\left(\tensor{\theta}{}\right) P_{k,n}\left(\tensor{\phi}{}\right)
        &\to
        \begin{bmatrix}
            1 & 0_{n \times1}          \\
            k\tensor{\theta}{} & I_n \\
        \end{bmatrix}\begin{bmatrix}
            1 & 0_{n \times1}          \\
            k\tensor{\phi}{} & I_n \\
        \end{bmatrix} && \text{(Equation above)} \\
        &=
        \begin{bmatrix}
            1 & 0_{n \times1}          \\
            k\tensor{\theta}{}+k\tensor{\phi}{} & I_n \\
        \end{bmatrix} && \text{(\cref{Matrix:Product:Block})}
    \end{align*}
\end{proof}
\begin{proposition}
    Group of orienatation with multiplication is isomorphic to orthogonal group.
    \begin{equation*}
        \left(Q\left(n\right),\cdot\right)\cong O\left(n\right)
    \end{equation*}
\end{proposition}
\begin{proposition}
    Group of point with multiplication is isomorphic to orthogonal group for $k>0$.
    \begin{equation*}
        \left(X\left(n\right),\cdot\right)\cong O\left(n\right)
    \end{equation*}
\end{proposition}
\begin{proposition}
    Group of point with multiplication is isomorphic to $\left(1,n\right)$-orthochronus indefinite orthogonal group for $k<0$.
    \begin{equation*}
        \left(X\left(n\right),\cdot\right)\cong O^+\left(1, n\right)
    \end{equation*}
\end{proposition}
\begin{proposition}
    Group of point with multiplication is isomorphic to Euclidean group for $k\to0$.
    \begin{equation*}
        \left(X\left(n\right),\cdot\right)\cong E\left(n\right)
    \end{equation*}
\end{proposition}
\section{Model Parametrization}
\begin{definition}\label{M:Parameter}
    For any point matrix $\tensor{X^\pm_{k,n}\left(\theta,\phi_1,\phi_2,\dots,\phi_n\right)}{}$,
    $n$-dimensional vector $\tensor{\theta}{}$
    is defined as \textit{position parameter}.
\end{definition}
\begin{definition}\label{M:Vector}
    For point matrix $\tensor{X}{}$,
    $\left(n+1\right)$-dimensional column vector $\tensor{p}{}\defeq \frac{1}{k} \tensor{X}{}\cdot\tensor{e}{^1}=\frac{1}{k} \tensor{X}{_1}$
    is defined as \textit{position vector}.
\end{definition}
\begin{definition}\label{M:Vector:Set}
    $P^\ast\left(k,n\right)$ is a set of position vectors.
\end{definition}
\begin{lemma}\label{M:Vector:Position}
    For point matrix $X=PO$
    where $P$ and $O$ are position and orientation matrix respectively,
    $\tensor{p}{}=\frac{1}{k}\tensor{X}{_1}=\frac{1}{k}\tensor{P}{_1}$.
\end{lemma}
\begin{proof}[\proofof{M:Vector:Position}]
    \begin{align*}
        \tensor{p}{^i}
         & = \frac{1}{k}\tensor{X}{^i_1}                         &  & \text{\cref{M:Vector}}       \\
         & = \frac{1}{k}\sum_j{\tensor{P}{^i_j}\tensor{O}{^j_1}} &  & \text{\cref{Matrix:Product}} \\
         & = \frac{1}{k}\tensor{P}{^i_1}                         &  & \text{\cref{M:Orientation}}  \\
        \tensor{p}{}                                                                               \\
         & = \frac{1}{k}\tensor{P}{_1}                           &  & \qedhere
    \end{align*}
\end{proof}
\begin{lemma}\label{M:Vector:Value}
    Given position parameter $\tensor{\theta}{}$, position vector can be evaluated as the following.
    \begin{equation*}
        \psi_0^{-1}: \tensor{\theta}{} \mapsto \tensor{p}{}
        = \frac{1}{k}
        \begin{pmatrix}
            \prod_{j\in\Set{1..n}}{\cos_k{\tensor{\theta}{^j}}}                                \\
            \sin_k{\tensor{\theta}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{\theta}{^j}}} \\
            \sin_k{{\tensor{\theta}{^n}}}                                                      \\
        \end{pmatrix}\text{.}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{M:Vector:Value}]
    Simplify \cref{M:Vector:Position,M:Position}.
\end{proof}
\begin{lemma}\label{M:Parameter:Value}
    Given position vector $\tensor{p}{}$, position parameter can be calculated as the following.
    \begin{align*}
        \psi_0
         & : \tensor{p}{} \mapsto \tensor{\theta}{}
        =
        \begin{pmatrix}
            \arcsin_k^{\sign{\tensor{p}{^1}}}{\frac{k\tensor{p}{^2}}{\prod_{j\in\Set{2..n}}{\cos_k{\tensor{\theta}{^j}}}}} \\
            \arcsin_k{\frac{k\tensor{p}{^{i+1}}}{\prod_{j\in\Set{i+1..n}}{\cos_k{\tensor{\theta}{^j}}}}}                   \\
            \arcsin_k{k\tensor{p}{^{n+1}}}                                                                                 \\
        \end{pmatrix} \\
         & \in
        \begin{cases}
            P \to \left(-\frac{\pi}{k}, \frac{\pi}{k}\right]\times\left[-\frac{1}{2}\frac{\pi}{k}, \frac{1}{2}\frac{\pi}{k}\right]^{n-1} & \text{if $k>0$}   \\
            P \to \R^{n}                                                                                                                 & \text{if $k\le0$} \\
        \end{cases}
    \end{align*}
    where $\cos_k\left(\arcsin_k^{\pm}\left(x\right)\right) = \pm \cos_k\left(\arcsin_k\left(x\right)\right)$.
\end{lemma}
\begin{proof}[\proofof{M:Parameter:Value}]
    From \cref{M:Vector:Value},
    \begin{align*}
        kp                                                                                 & = \begin{pmatrix}
                                                                                                   \prod_{j\in\Set{1..n}}{\cos_k{\tensor{\theta}{^j}}}                                \\
                                                                                                   \sin_k{\tensor{\theta}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{\theta}{^j}}} \\
                                                                                                   \sin_k{{\tensor{\theta}{^n}}}                                                      \\
                                                                                               \end{pmatrix}                \\
        \sin_k{{\tensor{\theta}{^n}}}                                                      & = k\tensor{\theta}{^{n+1}}                                                                                            \\
        \tensor{\theta}{^n}                                                                & = \arcsin_k{k\tensor{\theta}{^{n+1}}}                                                                                 \\
        \sin_k{\tensor{\theta}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{\theta}{^j}}} & = k\tensor{\theta}{^{i}}                                                                                              \\
        \sin_k{\tensor{\theta}{^{i-1}}}                                                    & = \frac{k\tensor{\theta}{^{i}}}{\prod_{j\in\Set{i..n}}{\cos_k{\tensor{\theta}{^j}}}}                                  \\
        \tensor{\theta}{^{i-1}}                                                            & = \arcsin_k{\frac{k\tensor{\theta}{^{i}}}{\prod_{j\in\Set{i..n}}{\cos_k{\tensor{\theta}{^j}}}}}                       \\
        \tensor{\theta}{^i}                                                                & = \arcsin_k{\frac{k\tensor{\theta}{^{i+1}}}{\prod_{j\in\Set{i+1..n}}{\cos_k{\tensor{\theta}{^j}}}}}                   \\
        \tensor{p}{^1}                                                                     & = \prod_{j\in\Set{1..n}}{\cos_k{\tensor{\theta}{^j}}}                                                                 \\
        \tensor{p}{^1}                                                                     & = \cos_k{\tensor{\theta}{^1}}\prod_{j\in\Set{2..n}}{\cos_k{\tensor{\theta}{^j}}}                                      \\
        \sign{\tensor{p}{^1}}                                                              & = \sign{\cos_k{\tensor{\theta}{^1}}}\prod_{j\in\Set{2..n}}{\sign{\cos_k{\tensor{\theta}{^j}}}}                        \\
        \sign{\tensor{p}{^1}}                                                              & = \sign{\cos_k{\tensor{\theta}{^1}}}\prod_{j\in\Set{2..n}}{1}                                                         \\
        \sign{\tensor{p}{^1}}                                                              & = \sign{\cos_k{\tensor{\theta}{^1}}}                                                                                  \\
        \theta                                                                             & = \begin{pmatrix}
                                                                                                   \arcsin_k^{\sign{\tensor{p}{^1}}}{\frac{k\tensor{p}{^2}}{\prod_{j\in\Set{2..n}}{\cos_k{\tensor{\theta}{^j}}}}} \\
                                                                                                   \arcsin_k{\frac{k\tensor{p}{^{i+1}}}{\prod_{j\in\Set{i+1..n}}{\cos_k{\tensor{\theta}{^j}}}}}                   \\
                                                                                                   \arcsin_k{k\tensor{p}{^{n+1}}}                                                                                 \\
                                                                                               \end{pmatrix}
    \end{align*}
\end{proof}
\begin{lemma}\label{M:CoordinateChart}
    \begin{equation*}
        \Psi=\set{\psi|
            \psi^{-1}
            \in S^n \to P
            :\tensor{\theta}{}\mapsto P_{k,n}\left(\tensor{\theta}{}+\tensor{x}{}\right)
            \text{ for }
            \tensor{x}{} \in \R^n
        }
    \end{equation*} is a coordinate chart of a $C^\infty$ differential structure on $P$
    for \begin{equation*}
        S=
        \begin{cases}
            \left(-\frac{1}{2}\frac{\pi}{k},+\frac{1}{2}\frac{\pi}{k}\right) & \text{$k>0$}   \\
            \R                                                               & \text{$k\le0$}
        \end{cases}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{M:CoordinateChart}]
    From \cref{Manifold}, It is sufficient to shows that
    \begin{APAenumerate}
        \item $R_\psi$ is an open subset of real vector space (defined),
        \item $\bigcup_{\psi\in\Psi} D_\psi = P$ (obvious),
        \item transition map is in differentability class $C^\infty$.
    \end{APAenumerate}
    \begin{subproof}{$\bigcup_{\psi\in\Psi} D_\psi = P$}
        \begin{align*}
            M \in P
                                         & \implies \exists \theta_0, M=P_{k,n}\left(\theta_0\right)                  \\
                                         & \implies \exists \theta_0, M=P_{k,n}\left(0 + \theta_0\right)              \\
                                         & \implies M\in R_{\psi^{-1}}                                                \\
                                         & \implies M\in D_{\psi}                                                     \\
                                         & \implies M\in \bigcup_{\psi\in\Psi} D_\psi                                 \\
            P                            & \subset\bigcup_{\psi\in\Psi}D_\psi                                         \\
            M\in \bigcup_{\psi\in\Psi} D_\psi
                                         & \implies \exists \psi\in\Psi, M\in D_\psi                                  \\
                                         & \implies \exists \psi\in\Psi, M\in R_{\psi^{-1}}                           \\
                                         & \implies \exists x_0\exists\theta\in S^n, M=P_{k,n}\left(\theta+x_0\right) \\
                                         & \implies \exists x_0, M=P_{k,n}\left(0+x_0\right)                          \\
                                         & \implies \exists x_0, M=P_{k,n}\left(x_0\right)                            \\
                                         & \implies M \in P                                                           \\
            \bigcup_{\psi\in\Psi}D_\psi  & \subset P                                                                  \\
            \bigcup_{\psi\in\Psi} D_\psi & = P
        \end{align*}
    \end{subproof}
    \begin{subproof}{every transition map is in differentability class $C^\infty$}
        Consider $\psi_1, \psi_2 \in \Psi$ and $x_1, x_2 \in \R^n$ where
        \begin{equation*}
            \psi_i^{-1}
            \in S^n \to R
            :\tensor{\theta}{}\mapsto P\left(\tensor{\theta}{}+x_i\right)
            \text{.}
        \end{equation*}
        If $\psi_1^{-1}\left(\theta_1\right)=\psi_2^{-1}\left(\theta_2\right)$,
        let $\phi_i = \theta_i+x_i$.
        \begin{align*}
            \frac{1}{k}
            \begin{pmatrix}
                \prod_{j\in\Set{1..n}}{\cos_k{\tensor{{\phi_1}}{^j}}}                                  \\
                \sin_k{\tensor{{\phi_1}}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{{\phi_1}}{^j}}} \\
                \sin_k{{\tensor{{\phi_1}}{^n}}}                                                        \\
            \end{pmatrix}
                                                                                                                      & =
            \frac{1}{k}
            \begin{pmatrix}
                \prod_{j\in\Set{1..n}}{\cos_k{\tensor{{\phi_2}}{^j}}}                                  \\
                \sin_k{\tensor{{\phi_2}}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{{\phi_2}}{^j}}} \\
                \sin_k{{\tensor{{\phi_2}}{^n}}}                                                        \\
            \end{pmatrix} & \text{\cref{M:Vector:Value}}        \\
            \begin{pmatrix}
                \prod_{j\in\Set{1..n}}{\cos_k{\tensor{{\phi_1}}{^j}}}                                  \\
                \sin_k{\tensor{{\phi_1}}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{{\phi_1}}{^j}}} \\
                \sin_k{{\tensor{{\phi_1}}{^n}}}                                                        \\
            \end{pmatrix}
                                                                                                                      & =
            \begin{pmatrix}
                \prod_{j\in\Set{1..n}}{\cos_k{\tensor{{\phi_2}}{^j}}}                                  \\
                \sin_k{\tensor{{\phi_2}}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{{\phi_2}}{^j}}} \\
                \sin_k{{\tensor{{\phi_2}}{^n}}}                                                        \\
            \end{pmatrix}
        \end{align*}

        If $k\ge 0$, by mathematical induction, $\phi_1 = m\frac{\pi}{k}\pm\phi_2$.

        Otherwise, by mathematical induction, $\phi_1 = \phi_2$.

        Hence, the transition map $\tau_{1,2} = \psi_2 \circ \psi_1^{-1}$
        is in the form of $\theta \mapsto c\pm\theta$
        and is in differentability class $C^\infty$.
    \end{subproof}
\end{proof}
\subsection{Locus of position vector}
\begin{lemma}\label{SphericalLocus}
    For $k>0$, $P^\ast$ is a $\left(n+1\right)$-sphere of radius $k^{-1}$.
\end{lemma}
\begin{proof}[\proofof{SphericalLocus}]
    Simplify \cref{M:Vector:Value} using \cref{M:Trigonometry:Pythagorean}.
\end{proof}
\begin{lemma}\label{HyperbolicLocus}
    For $k<0$, $P^\ast$ is a forward sheet of a two-sheeted $\left(n+1\right)$-hyperboloid of radius $k^{-1}$.
\end{lemma}
\begin{proof}[\proofof{HyperbolicLocus}]
    Simplify \cref{M:Vector:Value} using \cref{M:Trigonometry:Pythagorean}.
\end{proof}
\begin{lemma}\label{EuclideanLocus}
    For $k\to0$, $P^\ast$ is a $n$-Euclidean manifold at infinity.
\end{lemma}
\begin{proof}[\proofof{EuclideanLocus}]
    Using limits.
\end{proof}
\section{Geometric properties}
\subsection{Embedding}
\begin{definition}\label{M:Embedding}
    Let $N=\left(Q,g\right)$ be a $n$-dimension Riemannian manifold
    on position parameter space with such inner product $g$ that
    the map $\cdot\mapsto\frac{1}{k}\cdot\tensor{e}{^1}\in P\to P^\ast$ is an isometric embedding to $\left(n+1\right)$-Euclidean manifold.
\end{definition}
\begin{lemma}\label{M:Tangent:Basis}
    Position parameter $\tensor{\theta}{^i}$ is associated to the following vector in position vector space.
    \begin{equation*}
        \frac{\partial}{\partial\tensor{\theta}{^i}} =
        \begin{pmatrix}
            -k\tan_k^\ast\tensor{\theta}{^i}\tensor{p}{^j}         \\
            k\frac{1}{\tan_k\tensor{\theta}{^i}}\tensor{p}{^{i+1}} \\
            0
        \end{pmatrix} \frac{\partial}{\partial\tensor{p}{^j}}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{M:Tangent:Basis}]
    \begin{align*}
        \frac{\partial}{\partial\tensor{\theta}{^i}}
                                                                                                        & = \frac{\partial\tensor{p}{^j}}{\partial\tensor{\theta}{^i}}\frac{\partial}{\partial\tensor{p}{^j}} \\
                                                                                                        & = \frac{\partial}{\partial\tensor{\theta}{^i}}
        \left[
            \frac{1}{k}
            \begin{pmatrix}
                \prod_{l\in\Set{1..n}}{\cos_k{\tensor{\theta}{^l}}}                                \\
                \sin_k{\tensor{\theta}{^{j-1}}}\prod_{l\in\Set{j..n}}{\cos_k{\tensor{\theta}{^l}}} \\
                \sin_k{{\tensor{\theta}{^n}}}                                                      \\
            \end{pmatrix}
        \right]\frac{\partial}{\partial\tensor{p}{^j}}                                                  & \text{\cref{M:Vector:Value}}                                                                        \\
                                                                                                        & =
        \frac{1}{k}
        \frac{\partial}{\partial\tensor{\theta}{^i}}
        \left[
            \begin{pmatrix}
                \prod_{l\in\Set{1..n}}{\cos_k{\tensor{\theta}{^l}}}                                \\
                \sin_k{\tensor{\theta}{^{j-1}}}\prod_{l\in\Set{j..n}}{\cos_k{\tensor{\theta}{^l}}} \\
                \sin_k{{\tensor{\theta}{^n}}}                                                      \\
            \end{pmatrix}
        \right]\frac{\partial}{\partial\tensor{p}{^j}}                                                                                                                                                        \\
                                                                                                        & =
        \frac{1}{k}
        \left[
            \begin{pmatrix}
                -k\sin_k^\ast{\tensor{\theta}{^i}}\prod_{l\in\Set{1..n}/\Set{i}}{\cos_k{\tensor{\theta}{^l}}}                                \\
                -k\sin_k^\ast{\tensor{\theta}{^i}}\sin_k{\tensor{\theta}{^{j-1}}}\prod_{l\in\Set{j..n}/\Set{i}}{\cos_k{\tensor{\theta}{^l}}} \\
                k\cos_k{\tensor{\theta}{^{i}}}\prod_{l\in\Set{i+1..n}}{\cos_k{\tensor{\theta}{^l}}}                                          \\
                0                                                                                                                            \\
            \end{pmatrix}
        \right]\frac{\partial}{\partial\tensor{p}{^j}}                                                  & \text{\cref{M:Trigonometry:Derivative}}                                                             \\
                                                                                                        & =
        \begin{pmatrix}
            -\tan_k^\ast{\tensor{\theta}{^i}}\prod_{l\in\Set{1..n}}{\cos_k{\tensor{\theta}{^l}}}                                \\
            -\tan_k^\ast{\tensor{\theta}{^i}}\sin_k{\tensor{\theta}{^{j-1}}}\prod_{l\in\Set{j..n}}{\cos_k{\tensor{\theta}{^l}}} \\
            \frac{1}{\tan_k{\tensor{\theta}{^{i}}}}\prod_{l\in\Set{i+1..n}}{\cos_k{\tensor{\theta}{^l}}}                        \\
            0                                                                                                                   \\
        \end{pmatrix}
        \frac{\partial}{\partial\tensor{p}{^j}}                                                                                                                                                               \\
                                                                                                        & =
        \begin{pmatrix}
            -k\tan_k^\ast\tensor{\theta}{^i}\tensor{p}{^j}         \\
            k\frac{1}{\tan_k\tensor{\theta}{^i}}\tensor{p}{^{i+1}} \\
            0
        \end{pmatrix} \frac{\partial}{\partial\tensor{p}{^j}} & \text{\cref{M:Vector:Value}}                                                                                                          \\
    \end{align*}
\end{proof}
\begin{lemma}\label{M:MetricTensor}
    The metric tensor of $N$ is
    \begin{align*}
        \tensor{g}{_i_j} & =
        \begin{cases}
            \left(1-\sign{k}\right)
            \tan_k^2\left(\tensor{\theta}{^a}\right)
            \prod_{1\le j}{\cos_k^2\tensor{\theta}{^j}}
            +
            \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}} & \text{if $i=j$,}  \\
            0                                        & \text{otherwise.} \\
        \end{cases}
    \end{align*}
\end{lemma}
\begin{proof}[\proofof{M:MetricTensor}]
    \begin{align*}
        \tensor{g}{_a_b}\left[\frac{\partial}{\partial\tensor{\theta}{^i}}\right]
         & = \sum_{l,m=1}^{n+1}{
        \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^a}}}
        \tensor{g}{_l_m}\left[\frac{\partial}{\partial\tensor{p}{^i}}\right]
        \frac{\partial{\tensor{p}{^m}}}{\partial{\tensor{\theta}{^b}}}
        }                        \\
         & = \sum_{l=1}^{n+1}{
        \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^a}}}
        \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^b}}}
        }
    \end{align*}

    If $a<b$,
    \begin{align*}
        \tensor{g}{_a_b}
                         & =\begin{bmatrix}
                                -k\tan_k^\ast\left(\tensor{\theta}{^a}\right)\tensor{p}{^j}         \\
                                k\frac{1}{\tan_k\left(\tensor{\theta}{^a}\right)}\tensor{p}{^{a+1}} \\
                                0                                                                   \\
                            \end{bmatrix}\cdot\begin{bmatrix}
                                                  -k\tan_k^\ast\left(\tensor{\theta}{^b}\right)\tensor{p}{^j}         \\
                                                  k\frac{1}{\tan_k\left(\tensor{\theta}{^b}\right)}\tensor{p}{^{b+1}} \\
                                                  0                                                                   \\
                                              \end{bmatrix} \\
                         & =
        \sum{
            k^2\tan_k^\ast\left(\tensor{\theta}{^a}\right)\tan_k^\ast\left(\tensor{\theta}{^b}\right){\tensor{p}{^j}}^2
        }
        -k^2\frac{\tan_k^\ast\left(\tensor{\theta}{^b}\right)}{\tan_k\left(\tensor{\theta}{^a}\right)}{\tensor{p}{^{a+1}}}^2               \\
                         & =
        \tan_k\left(\tensor{\theta}{^b}\right)
        \tan_k\left(\tensor{\theta}{^a}\right)
        \prod_{a\le j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        -\sign{k}
        \right)                                                                                                                            \\
                         & =
        \tan_k\left(\tensor{\theta}{^b}\right)
        \tan_k\left(\tensor{\theta}{^a}\right)
        \prod_{a\le j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sign{k}\left(
        1-\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        \right)
        - \sign{k}
        \right)                                                                                                                            \\
        \tensor{g}{_a_b} & = 0 \text{.}
    \end{align*}

    If $a>b$, $\tensor{g}{_a_b}=\tensor{g}{_b_a}=0$.

    If $a=b$,
    \begin{align*}
        \tensor{g}{_a_a}
         & =\begin{pmatrix}
                -k\tan_k^\ast\tensor{\theta}{^a}\tensor{p}{^j}         \\
                k\frac{1}{\tan_k\tensor{\theta}{^a}}\tensor{p}{^{a+1}} \\
                0
            \end{pmatrix}\cdot\begin{pmatrix}
                                  -k\tan_k^\ast\tensor{\theta}{^a}\tensor{p}{^j}         \\
                                  k\frac{1}{\tan_k\tensor{\theta}{^a}}\tensor{p}{^{a+1}} \\
                                  0
                              \end{pmatrix} \\
         & =
        \sum_{j=1}^{a}{\left(k\tensor{p}{^j}\tan_k^\ast\tensor{\theta}{^a}\right)^2}
        +\left(k\tensor{p}{^{a+1}}\cot_k\tensor{\theta}{^a}\right)^2                                                       \\
         & =
        \sum_{j=1}^{a}{\left(k\tensor{p}{^j}\tan_k\tensor{\theta}{^a}\right)^2}
        +\left(k\tensor{p}{^{a+1}}\cot_k\tensor{\theta}{^a}\right)^2                                                       \\
         & =
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum_{1\le i<a}{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        +\cot_k^2\left(\tensor{\theta}{^a}\right)
        \right)                                                                                                            \\
         & =
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum_{1\le i<a}{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        +\cot_k^2\left(\tensor{\theta}{^a}\right)
        \right)                                                                                                            \\
         & =
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        -\sign{k}\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        +\sign{k}\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum_{1\le i<a}{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        +\cot_k^2\left(\tensor{\theta}{^a}\right)
        \right)                                                                                                            \\
         & =
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        -\sign{k}\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        +\sign{k}
        +\cot_k^2\left(\tensor{\theta}{^a}
        \right)
        \right)                                                                                                            \\
         & =
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        -\sign{k}\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        +\sign{k}
        +\csc_k^2\left(\tensor{\theta}{^a}
        -\sign{k}
        \right)
        \right)                                                                                                            \\
         & =
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \left(1-\sign{k}\right)\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        +\csc_k^2\left(\tensor{\theta}{^a}
        \right)
        \right)                                                                                                            \\
         & =
        \left(1-\sign{k}\right)
        \tan_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{j}{\cos_k^2\tensor{\theta}{^j}}
        +
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
    \end{align*}
\end{proof}
\subsection{Curvature}
\subsubsection{Curvature (Method I)}
This method may be easier to generalize to
Model II where each direction can have partially independent curvature
(or even Model III where extrinsic curvature become a thing).
But it may be challenging to define Gauss map properly.
\begin{lemma}\label{M:Normal}
    Given a position parameter $\tensor{\theta}{}$,
    the tangent vector in position vector space can be calculated as follows
    \begin{equation*}
        \nu(p)
        =
        \begin{cases}
            \begin{bmatrix}
                k\tensor{p}{^1}  \\
                +k\tensor{p}{^i} \\
            \end{bmatrix} & k>0     \\
            \begin{bmatrix}
                1 \\
                0 \\
            \end{bmatrix}     & k=0 \\
            \frac{1}{\sqrt{-1+2\left(k\tensor{p}{^1}\right)^2}}
            \begin{bmatrix}
                k\tensor{p}{^1}  \\
                -k\tensor{p}{^i} \\
            \end{bmatrix} & k<0
        \end{cases}
        \text{.}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{M:Normal}]
    From \cref{SphericalLocus,EuclideanLocus,HyperbolicLocus},
    \begin{equation*}
        p \in P \iff
        \begin{cases}
           \sum_{i}{{\tensor{p}{^i}}^2}=k^{-2} & \text{$k>0$,} \\
           \tensor{p}{^1} = k^{-1} \iffalse{\wedge \tensor{p}{^i} = \tensor{\theta}{^{i-1}}}\fi & \text{$k\to 0$,} \\
             {\tensor{p}{^1}}^2 - \sum_{1<i}{{\tensor{p}{^i}}^2}=k^{-2} \wedge \tensor{p}{^1} > 0 & \text{$k<0$.} \\
        \end{cases}
    \end{equation*}
    Let
    \begin{equation*}
        F(p) = \begin{cases}
           \sum_{i}{{\tensor{p}{^i}}^2}-k^{-2} & \text{$k>0$,} \\
           \tensor{p}{^1} - k^{-1} & \text{$k\to 0$,} \\
             {\tensor{p}{^1}}^2 - \sum_{1<i}{{\tensor{p}{^i}}^2}-k^{-2} & \text{$k<0$.} \\
        \end{cases}
    \end{equation*}
    \begin{align*}
        n
        &= l \nabla F(p) \\
        &= l \begin{cases}
            \nabla{\sum_{i}{{\tensor{p}{^i}}^2}-k^{-2}} & \text{$k>0$,} \\
            \nabla{\tensor{p}{^1} - k^{-1}} & \text{$k\to 0$,} \\
            \nabla{{\tensor{p}{^1}}^2 - \sum_{1<i}{{\tensor{p}{^i}}^2}-k^{-2}} & \text{$k<0$.} \\
         \end{cases} \\
         &= l \begin{cases}
            \begin{pmatrix}
                2\tensor{p}{^i} \\
            \end{pmatrix} & \text{$k>0$,} \\
            \begin{pmatrix}
                1 \\
                0 \\
            \end{pmatrix} & \text{$k\to 0$,} \\
            \begin{pmatrix}
                -2\tensor{p}{^1} \\
                2\tensor{p}{^i} \\
            \end{pmatrix} & \text{$k<0$.} \\
          \end{cases} \\
          \hat{n}
          &= \pm
            \begin{cases}
            \frac{1}{\norm{p}}
             \begin{pmatrix}
                 \tensor{p}{^i} \\
             \end{pmatrix} & \text{$k>0$,} \\
             \begin{pmatrix}
                 1 \\
                 0 \\
             \end{pmatrix} & \text{$k\to 0$,} \\
             \frac{1}{\norm{p}}
             \begin{pmatrix}
                 -\tensor{p}{^1} \\
                 \tensor{p}{^i} \\
             \end{pmatrix} & \text{$k<0$.} \\
           \end{cases} \\
           &= \pm
             \begin{cases}
             \frac{1}{k^{-1}}
              \begin{pmatrix}
                  \tensor{p}{^i} \\
              \end{pmatrix} & \text{$k>0$,} \\
              \begin{pmatrix}
                  1 \\
                  0 \\
              \end{pmatrix} & \text{$k\to 0$,} \\
              \frac{1}{\sqrt{-1+2\left(k\tensor{p}{^1}\right)^2}}
              \begin{pmatrix}
                  -\tensor{p}{^1} \\
                  \tensor{p}{^i} \\
              \end{pmatrix} & \text{$k<0$.} \\
            \end{cases} \\
            \nu=\hat{n}
            &\defeq
              \begin{cases}
               \begin{pmatrix}
                   k\tensor{p}{^i} \\
               \end{pmatrix} & \text{$k>0$,} \\
               \begin{pmatrix}
                   1 \\
                   0 \\
               \end{pmatrix} & \text{$k\to 0$,} \\
               \frac{1}{\sqrt{-1+2\left(k\tensor{p}{^1}\right)^2}}
               \begin{pmatrix}
                   \tensor{p}{^1} \\
                   -\tensor{p}{^i} \\
               \end{pmatrix} & \text{$k<0$.} \\
             \end{cases} \qedhere
    \end{align*}
\end{proof}
\begin{lemma}\label{M:ShapeOperator}
    \begin{equation*}
        L(p) = \begin{cases}
            -k I & \text{$k>0$,} \\
            0 & \text{$k\to0$,} \\
            \frac{1}{\norm{p}} \left(\frac{1}{\norm{p}^2} p p^T \diag{\left(-1,1\dots,1\right)} - I\right) & \text{$k<0$.} \\
        \end{cases}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{M:ShapeOperator}]
    \begin{align*}
        \frac{\partial}{\partial\tensor{p}{^j}}{\frac{1}{\norm{p}}}
        &= \frac{\partial}{\partial\norm{p}}{\frac{1}{\norm{p}}}\frac{\partial}{\partial\tensor{p}{^j}}{\norm{p}} \\
        &= {-\frac{1}{\norm{p}^2}}\frac{\partial}{\partial\tensor{p}{^j}}{\sqrt{\sum_i{{\tensor{p}{^i}}^2}}} \\
        &= {-\frac{1}{\norm{p}^2}}\frac{\partial}{\partial \sum_i{{\tensor{p}{^i}}^2}}{\sqrt{\sum_i{{\tensor{p}{^i}}^2}}}\frac{\partial}{\partial\tensor{p}{^j}}{\sum_i{{\tensor{p}{^i}}^2}} \\
        &= {-\frac{1}{\norm{p}^2}}\frac{1}{2\sqrt{\sum_i{{\tensor{p}{^i}}^2}}}{2{{\tensor{p}{^j}}}} \\
        &= {-\frac{1}{\norm{p}^2}}\frac{1}{2\norm{p}}{2{{\tensor{p}{^j}}}} \\
        &= -\frac{\tensor{p}{^j}}{\norm{p}^3}
    \end{align*}
    From \cref{Manifold:Hypersurface},
    \begin{align*}
        L(p)
        &= - (D\nu \circ \left(Df\right)^{-1})(p) \\
        &= - (D\nu)(p) \\
        &= - \left(\left.\frac{\partial}{\partial\tensor{p}{^j}}\tensor{\nu}{^i}\right|_p\right)_{i,j} \\
        \left(L(p)\right)_{i,j}
        &= \begin{cases}
            - \frac{\partial}{\partial\tensor{p}{^j}} k\tensor{p}{^i} & \text{$k>0$,} \\
            - \frac{\partial}{\partial\tensor{p}{^j}} 1 & \text{$k\to 0$ and $i=1$,} \\
            - \frac{\partial}{\partial\tensor{p}{^j}} 0 & \text{$k\to 0$ and $i\ne1$,} \\
            - \frac{\partial}{\partial\tensor{p}{^j}} \left({\frac{1}{\norm{p}}}\tensor{p}{^1}\right) & \text{$k<0$ and $i=1$,} \\
            - \frac{\partial}{\partial\tensor{p}{^j}} \left(-{\frac{1}{\norm{p}}}\tensor{p}{^i}\right) & \text{$k<0$ and $i\ne 1$,} \\
          \end{cases} \\
          &= \begin{cases}
              - k & \text{$k>0$ and $i=j$,} \\
              0 & \text{$k>0$ and $i\ne j$,} \\
              0 & \text{$k\to 0$,} \\
              - \left({\frac{1}{\norm{p}}}\frac{\partial}{\partial\tensor{p}{^j}}\tensor{p}{^1} + \tensor{p}{^1}\frac{\partial}{\partial\tensor{p}{^j}}{\frac{1}{\norm{p}}}\right) & \text{$k<0$ and $i=1$,} \\
              \left({\frac{1}{\norm{p}}}\frac{\partial}{\partial\tensor{p}{^j}}\tensor{p}{^i} + \tensor{p}{^i}\frac{\partial}{\partial\tensor{p}{^j}}{\frac{1}{\norm{p}}}\right) & \text{$k<0$ and $i\ne 1$,} \\
            \end{cases} \\
            &= \begin{cases}
                - k & \text{$k>0$ and $i=j$,} \\
                0 & \text{$k>0$ and $i\ne j$,} \\
                0 & \text{$k\to 0$,} \\
                -{\frac{1}{\norm{p}}} + \frac{{\tensor{p}{^1}}^2}{\norm{p}^3} & \text{$k<0$ and $i=j=1$,} \\
                \frac{\tensor{p}{^1}\tensor{p}{^j}}{\norm{p}^3} & \text{$k<0$ and $j\ne i=1$,} \\
                {\frac{1}{\norm{p}}} - \frac{{\tensor{p}{^i}}^2}{\norm{p}^3} & \text{$k<0$ and $i=j\ne1$,} \\
                -\frac{\tensor{p}{^i}\tensor{p}{^j}}{\norm{p}^3} & \text{$k<0$, $i\ne j$ and $i\ne1$,} \\
              \end{cases}
    \end{align*}

    For $k>0$, $L(p)=-k I$.

    For $k\to0$, $L(p)=0$.

    For $k<0$, $L(p)= \frac{1}{\norm{p}} \left(\frac{1}{\norm{p}^2} p p^T \diag{\left(-1,1\dots,1\right)} - I\right)$.
\end{proof}
\begin{lemma}\label{Model:PrincipalCurvature}

\end{lemma}
\begin{proof}[\proofof{Model:PrincipalCurvature}]
    \skipped

    For $k>0$,
    \begin{align*}
        0
        &= \det{L(p)-\lambda I} \\
        &= \det{-k I-\lambda I} \\
        &= \det{(-k-\lambda) I} \\
        &= (-k-\lambda)^{n+1}\det{I} \\
        &= (-k-\lambda)^{n+1} \\
        \lambda &\in \Set{-k,\dots,-k}
    \end{align*}

    For $k<0$,
    \begin{align*}
        0
        &= \det{L(p)-\lambda I} \\
        &= \det{\frac{1}{\norm{p}} \left(\frac{1}{\norm{p}^2} p p^T \diag{\left(-1,1\dots,1\right)} - I\right)-\lambda I} \\
        &= \det{\frac{1}{\norm{p}} \left(\frac{1}{\norm{p}^2} p p^T \diag{\left(-1,1\dots,1\right)} - I\right)-\left(\lambda^\prime-\frac{1}{\norm{p}}\right) I} & \text{$\lambda^\prime \defeq \lambda + \frac{1}{\norm{p}}$}\\
        &= \det{\frac{1}{\norm{p}^3} p p^T \diag{\left(-1,1\dots,1\right)} - \frac{1}{\norm{p}}I-\left(\lambda^\prime-\frac{1}{\norm{p}}\right) I}\\
        &= \det{\frac{1}{\norm{p}^3} p p^T \diag{\left(-1,1\dots,1\right)} - \lambda^\prime I}\\
        &= \det{\frac{1}{\norm{p}^3} p p^T \diag{\left(-1,1\dots,1\right)} - \frac{1}{\norm{p}^3}\lambda^{\prime\prime} I} & \text{$\lambda^{\prime\prime} \defeq \norm{p}^3\lambda^\prime$}\\
        &= \frac{1}{\norm{p}^{3(n+1)}}\det{p p^T \diag{\left(-1,1\dots,1\right)} - \lambda^{\prime\prime} I}\\
        &= \det{p p^T \diag{\left(-1,1\dots,1\right)} - \lambda^{\prime\prime} I}\\
        &= {\lambda^{\prime\prime}}^n\left(\lambda^{\prime\prime}+\tensor{p}{^1}^2-\sum_{i\ne 1}{\tensor{p}{^i}^2}\right) \\
        0 &= {\lambda^{\prime\prime}}^n\left(\lambda^{\prime\prime}+\frac{1}{k}\right) \\
        \lambda^{\prime\prime} &\in \Set{-\frac{1}{k},0,\dots,0} \\
        \lambda^\prime &\in \Set{-\frac{1}{k\norm{p}^3},0,\dots,0} \\
        \lambda &\in \Set{-\frac{1}{k\norm{p}^3} - \frac{1}{\norm{p}},-\frac{1}{\norm{p}},\dots,-\frac{1}{\norm{p}}} \\
        &= \Set{-\frac{1+\norm{p}^2}{k\norm{p}^3},-\frac{1}{\norm{p}},\dots,-\frac{1}{\norm{p}}} \\
        &= \Set{-\frac{2k^2{\tensor{p}{^1}}^2}{k\norm{p}^3},-\frac{1}{\norm{p}},\dots,-\frac{1}{\norm{p}}} \\
        &= \Set{-\frac{2k{\tensor{p}{^1}}^2}{\norm{p}^3},-\frac{1}{\norm{p}},\dots,-\frac{1}{\norm{p}}} \\
        &= \Set{k^2\norm{p}\frac{-2{\tensor{p}{^1}}^2}{k\norm{p}^4},-\frac{1}{\norm{p}},\dots,-\frac{1}{\norm{p}}} \\
        &= \Set{k^2\norm{p}\frac{-2{\tensor{p}{^1}}^2}{k\norm{p}^4},-\frac{1}{\norm{p}},\dots,-\frac{1}{\norm{p}}} \\
    \end{align*}
\end{proof}
\subsubsection{Curvature (Method II)}
This method may be easier to be done
(despite the fact that it never finished).
But it raises problems when trying to generalize e.g. dealing with extrinsic curvature
(which may be introduced in Model III
if not to mess with other basis geometries).
\begin{lemma}\label{Model:ChristoffelSymbol}

\end{lemma}
\begin{lemma}\label{Model:RiemannCurvatureTensor}

\end{lemma}
\subsubsection{Curvature (Conclusion)}
\begin{lemma}\label{Model:SectionalCurvature}

\end{lemma}
\paragraph{CurvatureParameter}
It can be seen that $\operatorname{sec}\left(p\right) = \kappa = \sign{\left(k\right)}k^2$.
Hence, when provided $\kappa$, $k$ can be determined and used to evaluate the model.
\section{The Model}
\stepcounter{Counter}
\begin{ModelGroupElement}
    For any parameter $\kappa, n$,
    \begin{align*}
        \elements                   & \defeq M                                    \\
        \groupoperation{\elements}  & \defeq \cdot                                \\
        \charts{\elements}          & \defeq \tensor{X}{}\mapsto\tensor{\theta}{} \\
        \innerprod{\elements}       & \defeq g                                    \\
        \points{\elements}          & \equiv R                                    \\
        \transformations{\elements} & \equiv M                                    \\
    \end{align*}
    for injective smooth function $K:\kappa \mapsto k = \sign{\kappa}\sqrt{\abs{\kappa}}\in\R\to\R$.
\end{ModelGroupElement}
\begin{ModelGroupAssertion}
\end{ModelGroupAssertion}
\begin{ModelCurvatureAssertion}
\end{ModelCurvatureAssertion}
\section{Future plan}
\subsection{Model II}
It is very simple to be able to model composite geometries e.g. $S^2 \times E$ by tensor product of the existing model. But to be able to merge them as smooth model may be challenging since not all combination of basis curvature have their own intrinsic geometry. So it may be to find independent variable for each basis or to introduce extrinsic curvature (Model A).
\subsection{Model B}
It is known that $E$ emerged at $n\ge1$ while $S$ and $H$ emerged at $n\ge2$ and there's more complex pure geometries than these that emerged in higher dimension. It is interesting and challenging to explore such geometries and prove whether the curvature still works as indicator in such geometries or are there any patterns for their symmetries.
\subsection{Model A}
This model is based on curvature and mostly just 3 basis geometries and extrinsic curvature which seems to be interesting despite some critical result in some combination e.g. $S^1 \times S^1$ vs $S^2$. It can be even more challenging to have variable curvature with respect to other intrinsic position.
\section*{}
\printbibliography
\begin{figure}
    \centering
    \insertstandalone{figures}{2dplot}
    \caption{Generalized trigonometric functions as function of $k$}\label{TrigonometryPlotted}
    \figurenote{This graph shows the value of generalized trigonometric functions as solid line and trigonometric and hyperbolic functions in the unused domain as dashed line.}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{sine}
    \caption{Generalized sine function}\label{TrigonometrySinePlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{sine_}
    \caption{Generalized sine function variant}\label{TrigonometrySineVarPlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{cosine}
    \caption{Generalized cosine function}\label{TrigonometryCosinePlotted}
\end{figure}
\end{document}

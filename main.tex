%!TeX spellcheck = en_US
%!TeX encoding = utf8
%!TeX root = main.tex
%!TeX program = pdflatex

% https://apastyle.apa.org/style-grammar-guidelines
% https://www.ams.org/publications/authors/AMS-StyleGuide-online.pdf
\documentclass[stu, babel, american, biblatex, a4paper, leqno, draftall]{apa7}
\usepackage{csquotes}
\addbibresource{lists/preliminary.bib}
\DeclareLanguageMapping{american}{american-apa}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{tensor}
\usepackage{braket}
\usepackage{hyperref}
\usepackage[capitalise, sort&compress, noabbrev, nameinlink]{cleveref}

\iffalse % memory exceeded, include precompiled .pdf instead.
\iffalse % standalone subpreambles invoke errors, manually import instead.
\usepackage[subpreambles=true]{standalone}
\else
\usepackage{standalone}

\usepackage{pgf-umlcd}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

% \usetikzlibrary{external}
% \tikzexternalize
\fi
\usepackage{import}
\newcommand{\insertstandalone}[2]{\import{#1}{#2}}
\else
\newcommand{\insertstandalone}[2]{\includegraphics[width=\textwidth]{#1/#2.pdf}}
\fi

\allowdisplaybreaks
\crefname{paragraph}{paragraph}{paragraphs}
\Crefname{paragraph}{Paragraph}{Paragraphs}
\crefname{subparagraph}{subparagraph}{subparagraphs}
\Crefname{subparagraph}{Subparagraph}{Subparagraphs}


\title{Prove of Constant Radius Model}
\shorttitle{30MA19-02}
\leftheader{Hanchai, Sakepisit}
\authorsnames{Hanchai Nonprasart, Sakepisit Maysamat}
\authorsaffiliations{Mahidol Wittayanusorn School}
\course{SCI 30196: Science Project 1}
\professor{Amornsri Amornvatcharapong, Teerapong Suksumran}
\duedate{\today} % Last edit day when the draft watermark is removed.
\abstract{This is the first generation of the model. It's a draft. Who would even care to read the abstract when it's not done anyways?}
\keywords{None 1, None 2, None 3}

\input{lists/notations.tex}
\input{lists/theories.tex}

\begin{document}
\maketitle
\tableofcontents

\section{Rewrite note}
This proof will be organized using the following guideline
\begin{APAenumerate}
    \item reviewing existing definitions and theorem
    \item proving certain property of the existing mathematical objects
    \item defining the conditions of the model
    \item formulating of the model
    \item parameterize the model
    \item asserting defined property of the model
\end{APAenumerate}
It turned out that figures will crash overleaf, if you want to view such file, please compile them seperately.
To merge the file, it is needed to either pay the subscription or download and run it on your computer.

\section{Preliminary}

\subsection{Tensor}

\begin{equation}\label{Tensor:Sum}
    \tensor{\left(A+B\right)}{
        ^{i_1}^{\dots}^{i_n}
        _{j_1}_{\dots}_{j_m}
    }
    =
    \tensor{A}{
        ^{i_1}^{\dots}^{i_n}
        _{j_1}_{\dots}_{j_m}
    }
    +
    \tensor{B}{
        ^{i_1}^{\dots}^{i_n}
        _{j_1}_{\dots}_{j_m}
    }
\end{equation}
\begin{equation}\label{Tensor:Product:Scalar}
    \tensor{\left(\alpha A\right)}{
        ^{i_1}^{\dots}^{i_n}
        _{j_1}_{\dots}_{j_m}
    }
    =
    \alpha
    \tensor{A}{
        ^{i_1}^{\dots}^{i_n}
        _{j_1}_{\dots}_{j_m}
    }
\end{equation}
\begin{equation}\label{Tensor:Product:Tensor}
    \tensor{\left(A\otimes B\right)}{
        ^{i_1}^{\dots}^{i_l}^{i_{l+1}}^{\dots}^{i_{l+n}}
        _{j_1}_{\dots}_{j_k}_{j_{k+1}}_{\dots}_{j_{k+m}}
    }
    =
    \tensor{A}{
        ^{i_1}^{\dots}^{i_l}
        _{j_1}_{\dots}_{j_k}
    }
    \tensor{B}{
        ^{i_{l+1}}^{\dots}^{i_{l+n}}
        _{j_{k+1}}_{\dots}_{j_{k+m}}
    }
\end{equation}
\begin{equation}\label{Tensor:Contraction}
    \tensor{\left(\contr{\tensor{T}{}}\right)}{
        ^{i_1}^{\dots}^{i_n}
        _{j_1}_{\dots}_{j_m}
    }
    =
    \sum_a{\tensor{T}{
        ^{i_1}^{\dots}^{i_n}^a
        _a_{j_1}_{\dots}_{j_m}
    }}
\end{equation}
\begin{equation}\label{Matrix:Product}
    \tensor{\left(AB\right)}{^i_j}
    =\contr{\left(A\otimes B\right)}
    =\sum_k\tensor{A}{^i_k}\tensor{B}{^k_j}
\end{equation}
\begin{equation}\label{Matrix:Product:Block}
    \left(\begin{bmatrix}
        A_{11} & A_{12} \\
        A_{21} & A_{22} \\
    \end{bmatrix}
    \begin{bmatrix}
        B_{11} & B_{12} \\
        B_{21} & B_{22} \\
    \end{bmatrix}\right)_{ij}
    =
    \sum_k A_{ik} B_{kj}
\end{equation}
\begin{equation}\label{Matrix:Identity}
    IA = A = AI
\end{equation}
\begin{equation}\label{Matrix:Identity:Value}
    I_n=\diag{\left(1,1,\dots,1\right)}
\end{equation}
\begin{equation}\label{Matrix:Identity:Block}
    I_{a+b}=
    \begin{bmatrix}
    I_a & 0_{a\times b} \\
    0_{b\times a} & I_b \\
    \end{bmatrix}
\end{equation}
\begin{equation}\label{Matrix:Permutation:Square}
    T_{a,b} T_{a,b}= I
\end{equation}

\subsection{Group}

\begin{definition}[Lie group {\autocite[][Chapter~7]{lee_2013}}]\label{Group:Lie}
    A \textit{Lie group} is a smooth manifold $G$ (without boundary)
    that is also a group in the algebraic sense,
    with the property that
    the multiplication map $m:G\times G\to G$
    and the inversion map $i:G\to G$, given by
    \begin{equation*}
    m\left(g,h\right)=gh\text{,}
    \quad\quad
    i\left(g\right)=g^{-1}
    \end{equation*}
    are both smooth.
\end{definition}

\begin{proposition}[Lie group {\autocite[][Chapter~7]{lee_2013}}]\label{Group:Lie:Assertion}
If $G$ is a smooth manifold with a group structure such that the map $G\times G\to G$ given by $\left(g,h\right)\mapsto gh^{-1}$ is smooth, then $G$ is a Lie group.
\end{proposition}

\begin{definition}[Semidirect product]\label{Group:SemidirectProduct}
    Suppose $H$ and $N$ are groups,
    and $\theta:H\times N\to N$ is a smooth left action of $H$ on $N$.
    It is said to be an \textit{action by automorphisms} 
    if for each $h\in H$, the map $\theta_h:N\to N$ is a group automorphism of $N$ (i.e., an isomorphism from $N$ to itself).
    Given such action, we define a new group $N\rtimes_\theta H$, 
    called a \textit{semidirect product} of $H$ and $N$, as follows.
    $N\rtimes_\theta H$ is just the Cartesian product $N\times H$;
    but the group multiplication is defined by
    $$\left(n,h\right)\left(n^\prime,h^\prime\right)=\left(n\theta_h\left(n^\prime\right),hh^\prime\right)\text{.}$$
\end{definition}

\begin{definition}\label{KleinGeometry}
    A \textit{Klein geometry} is a pair $\left(G, H\right)$
    where $G$ is a Lie group
    and $H$ is a closed Lie subgroup of $G$
    such that the (left) coset space $$X\defeq G / H$$ is connected.
\end{definition}
\begin{example}\label{KleinGeometryExample}
    KleinGeometryExamples (I)
\end{example}

\subsection{Manifold}

\begin{definition}[Abstract differentiable manifold {\autocite[][Chapter~5A]{kuhnelwolfgang_2006}}]\label{Manifold}
    A \textit{$k$-dimensional differentiable manifold} (briefly: a $k$-manifold)
    is a set $M$ together with a family $\left(M_i\right)_{i\in I}$ of subsets such that
    \begin{APAenumerate}
        \item $M=\bigcup_{i\in I} M_i$ (union),
        \item for every $i\in I$ there is an injective map $\varphi_i:M_i\to\R^k$ so that $\phi_i\left(M_i\right)$ is open in $\R^k$, and
        \item for $M_i\cap M_j\ne\emptyset$, $\varphi_i\left(M_i\cap M_j\right)$ is open in $\R^k$.
    \end{APAenumerate}
\end{definition}

\begin{definition}[Structures on a manifold {\autocite[][Chapter~5A]{kuhnelwolfgang_2006}}]\label{Manifold:Extended}
    Given a $k$-dimensional differentiable manifold,
    one gets additional structure
    by replacing aditional requirements on the transformation functions $\varphi_j\circ\varphi_i^{-1}$,
    which belong to the atlas of the manifold;
    if all $\varphi_j\circ\varphi_i^{-1}$ are (left-hand side),
    then one speaks of (right-hand side) as follows:
    \begin{center}
    \begin{tabular}{ r c l }
    continuous & $\leftrightarrow$ & topological manifold \\
    differentiable & $\leftrightarrow$ & differentiable manifold \\
    $C^1$-differentiable & $\leftrightarrow$ & $C^1$-manifold \\
    $C^r$-differentiable & $\leftrightarrow$ & $C^r$-manifold \\
    $C^\infty$-differentiable & $\leftrightarrow$ & $C^\infty$-manifold \\
    real analytic & $\leftrightarrow$ & real analytic manifold \\
    complex analytic & $\leftrightarrow$ & complex analytic manifold of dimension $\frac{k}{2}$ \\
    affine & $\leftrightarrow$ & affine manifold \\
    projective & $\leftrightarrow$ & projective manifold \\
    conformal & $\leftrightarrow$ & manifold with a conformal structure \\
    orienatation-preserving & $\leftrightarrow$ & orientable manifold \\
    \end{tabular}
    \end{center}
\end{definition}

\begin{definition}[Tangent vector {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:TangentVector}
    A \textit{tangent vector} $X$ at $p$
    is a derivation (derivative operator) defined on the set of \textit{germs of functions}
    $$\mathcal{F}_p\left(M\right)\defeq\set{f:M\to\R|f\text{ differentiable}}/\sim\text{,}$$
    where the equivalence relation $\sim$ is defined by
    declaring $f\sim f^\ast$ if and only if
    $f$ and $f^\ast$ coincide in a neighborhood of $p$.
    The value $X\left(f\right)$ is also referred to as the \textit{directional derivative} of $f$ in the direction $X$.

    This definition means more precisely the following.
    $X$ is a map $X:\mathcal{F}_p\left(M\right)\to\R$
    with the two following properties:
    \begin{APAenumerate}
        \item $X\left(\alpha f+\beta g\right)=\alpha X\left(f\right)+\beta\left(g\right)$, $f,g\in\mathcal{F}_p\left(M\right)$ (\textit{$\R$-linearity});
        \item $X\left(f\cdot g\right)=X\left(f\right)\cdot g\left(p\right)+f\left(p\right)\cdot X\left(g\right)$ for $f,g\in\mathcal{F}_p\left(M\right)$ (\textit{product rule}).
    \end{APAenumerate}
    (For this to make sense, both $f$ and $g$ have to be defined in a neighborhood of $p$.)

    Briefly: \textit{tangent vectors are derivations acting on scalar functions.}
\end{definition}

\begin{definition}[Tangent space {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:TangentSpace}
    The \textit{tangent space} $T_pM$ of $M$ at $p$
    is defined in all cases as
    the set of all tangent vectors at the point $p$.
    By definition $T_pM$ and $T_qM$ are disjoint if $p\ne q$.
\end{definition}

\begin{definition}[Derivative {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:Derivative}
    Let $F:M\to N$ be a differentiable map,
    and let $p$, $q$ be two fixed point with $F\left(p\right)=q$.
    Then the \textit{derivative} or the $\textit{differential}$ of $F$ at $p$ is defined as the map
    $$\left.DF\right|_p:T_pM\to T_qN$$
    whose value at $X\in T_pM$ is given by
    $\left(\left.DF\right|_p\left(X\right)\right)\left(f\right)=X\left(f\circ F\right)$
    for every $f\in \mathcal{F}_q\left(N\right)$
    (which automatically implies the relation $f\circ F\in\mathcal{F}_p\left(M\right)$).
\end{definition}

\begin{lemma}[Chain rule {\autocite[][Chapter~5B]{kuhnelwolfgang_2006}}]\label{Manifold:ChainRule}
    For the derivative as defined in this manner, one has the \textit{chain rule} in the form
    $$\left.D\left(G\circ F\right)\right|_p=\left.DG\right|_{F\left(p\right)}\circ\left.DF\right|_p$$
    for every composition $M\xrightarrow{F}N\xrightarrow{G}Q$ of maps, or, more briefly, $D\left(G\circ F\right)=DG\circ DF$.
\end{lemma}

\begin{definition}[Riemannian metric {\autocite[][Chapter~5C]{kuhnelwolfgang_2006}}]\label{Manifold:RiemannianMetric}
    A \textit{Riemannian metric} $g$ on $M$
    is an association $p\mapsto g_p\in L^2\left(T_pM;\R\right)$
    such that the following conditins are satisfied:
    \begin{APAenumerate}
        \item $g_p\left(X,Y\right)=g_p\left(Y,X\right)$ for all $X$, $Y$, \hfill (\textit{symmetry})
        \item $g_p\left(X,X\right)>0$ for all $X\ne0$, \hfill (\textit{positive definiteness})
        \item The coefficient $\tensor{g}{_i_j}$ in every local representation (i.e., in every chart) $$g_p=\sum_{i,j}\tensor{g}{_i_j}\left(p\right)\cdot \left.d\tensor{x}{^i}\right|_p \otimes\left.d\tensor{x}{^j}\right|_p$$ are differentiable functions. \hfill (\textit{differentiability})
    \end{APAenumerate}
\end{definition}

\begin{remark}\label{Manifold:Riemannian}
    The pair $\left(M,g\right)$ is then called \textit{Riemannian manifold}.
    One also refers to the Riemannian metric as the \textit{metric tensor}.
    In local coordinates the metric tensor is given by the matrix ($\tensor{g}{_i_j}$) of functions.
    In Ricci calculus this is simply written as $\tensor{g}{_i_j}$.
    {\autocite[][Chapter~5C]{kuhnelwolfgang_2006}}
\end{remark}

\begin{remark}\label{Manifold:InnerProduct}
    A Riemannian metric $g$ defines at every point $p$
    an \textit{inner product} $g_p$ on the tangent space $T_pM$,
    and therefore the notation $\inner{X}{Y}$ instead of $g_p\left(X,Y\right)$ is also used.
    The notions of angles and lengths are determined by this inner product,
    just as these notions are determined by the first fundamental form on surface elements.
    The length or norm of vector $X$ is given by $\norm{X}\defeq\sqrt{g\left(X,X\right)}$,
    and the angle $\beta$ between two tangent vectors $X$ and $Y$
    can be defined by the validity of the equation $\cos\beta\cdot\norm{X}\cdot\norm{Y}=g\left(X,Y\right)$.
    {\autocite[][Chapter~5C]{kuhnelwolfgang_2006}}
\end{remark}

\subsection{Curvature}

\begin{example}\label{ShapeOperator}
    $$S\left(v\right)=\pm\nabla_v n$$
\end{example}
\begin{example}\label{PrincipalCurvature}
    Eigenvalue of second fundamental form (or shape operator)
\end{example}

\section{Objective}
\paragraph{Objective}\label{Objective}
The objective is that
given a natural number $n$ and a real number $\kappa$,
one can construct
\begin{APAenumerate}
    \item a set $M$
    \item a Lie group $M$ with operation $\otimes_M$,
    \item an $n$-dimensional $C^\infty$-manifold $M$ with chart $\varphi_i \subset M\to\R^n$,
    \item a Riemannian manifold $M$ with inner product $g_p\in T_pM\times T_pM\to\R$
\end{APAenumerate}
(to be determined)
such that
\begin{APAitemize}
    \item group action is distance preserved. 
    \item model is continuous with respect to $\kappa$ (and smooth with respect to each basis).
    \item for all two-dimensional linear subspaces of the manifold, the sectional curvature is $\kappa$.
\end{APAitemize}
(cannot figure formal definition out yet.)

\begin{conjecture}\label{GeometricGroupStructure}
    If the parameters $\left(\kappa,n\right)$ is associated with Klein geometry $\left(G,H\right)$
    then $\left(M,\otimes_M\right)\cong G/H$.

    That is, from \cref{KleinGeometryExample},
    \begin{APAitemize}
        \item For $\kappa>0$, $G\cong\ortho\left(n+1\right)$ and $H\cong\ortho\left(n\right)$.
        \item For $\kappa=0$, $G\cong\euclid\left(n\right)$ and $H\cong\ortho\left(n\right)$.
        \item For $\kappa<0$, $G\cong\ortho^{+}\left(1,n\right)$ and $H\cong\ortho\left(n\right)$.
    \end{APAitemize}
\end{conjecture}

\section{Model Foundation}
\proto{Construction of the set $M$ and operator $\otimes_M$}

\subsection{Trigonometry}
\begin{definition}\label{M:Trigonometry}
    \textit{Generalized trigonometric functions} $f_k:\R\to\R$ and $f_k^\ast:\R\to\R$ are defined as
    \begin{align*}
        f_k\left(\theta\right) & \defeq
        \begin{cases}
            g\left(k\theta\right) & \text{if $k\geq0$,} \\
            h\left(k\theta\right) & \text{otherwise,}   \\
        \end{cases} \\
        f_k^\ast\left(\theta\right) & \defeq
        \begin{cases}
            g\left(k\theta\right) & \text{if $k\geq0$,} \\
            h\left(-k\theta\right) & \text{otherwise,}   \\
        \end{cases} \\
        & =
        \begin{cases}
            g\left(k\theta\right) & \text{if $k\geq0$,} \\
            \pm h\left(k\theta\right) & \text{otherwise,}   \\
        \end{cases}
    \end{align*}
    where $g$ (resp. $h$) are the associated trigonometric (resp. hyperbolic) function.
\end{definition}
\begin{corollary}[Absolute value of generalized trigonometric functions]\label{M:Trigonometry:Absolute}
    \begin{equation*}
        \abs{f_k\left(\theta\right)}
        =
        \abs{f_k^\ast\left(\theta\right)}
    \end{equation*}
\end{corollary}
\begin{example}[Generalized sine functions]\label{M:Trigonometry:Sine}
    \begin{align*}
        \sin_k{\theta} & \defeq
        \begin{cases}
            \sin\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \sinh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases} \\
        \sin_k^\ast{\theta} & \defeq
        \begin{cases}
            \sin\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \sinh\left(-k\theta\right) & \text{otherwise.}   \\
        \end{cases} \\
        & =
        \begin{cases}
            \sin\left(k\theta\right)  & \text{if $k\geq0$,} \\
            -\sinh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}
    \end{align*}
    (see \cref{TrigonometrySinePlotted})
\end{example}
\begin{example}[Generalized cosine functions]\label{M:Trigonometry:Cosine}
    \begin{align*}
        \cos_k{\theta} & \defeq
        \begin{cases}
            \cos\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \cosh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases} \\
        \cos_k^\ast{\theta} & \defeq
        \begin{cases}
            \cos\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \cosh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases} \\
        & = \cos_k{\theta}
    \end{align*}
    (see \cref{TrigonometryCosinePlotted})
\end{example}
\begin{example}[Generalized tangent functions]\label{M:Trigonometry:Tangent}
    \begin{align*}
        \tan_k{\theta} & \defeq
        \begin{cases}
            \tan\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \tanh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases} \\
        \tan_k^\ast{\theta} & \defeq
        \begin{cases}
            \tan\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \tanh\left(-k\theta\right) & \text{otherwise.}   \\
        \end{cases} \\
        & =
        \begin{cases}
            \tan\left(k\theta\right)  & \text{if $k\geq0$,} \\
            -\tanh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}
    \end{align*}
    (see \cref{TrigonometryTangentPlotted})
\end{example}
\begin{theorem}[Pythagorean's identity equivalence]\label{M:Trigonometry:Pythagorean}
    \begin{align*}
        \cos_k^2{\theta} & +\sign{k}\sin_k^2{\theta} & = 1                \\
        1                & +\sign{k}\tan_k^2{\theta} & = \sec_k^2{\theta} \\
        \cot_k^2{\theta} & +\sign{k}                 & = \csc_k^2{\theta}
    \end{align*}
\end{theorem}
\begin{proof}[\proofof{M:Trigonometry:Pythagorean}]
    Proof by exhaustion.
\end{proof}
\begin{proposition}[Generalized trigonometric functions of sum of arguments]\label{M:Trigonometry:Sum}
    \begin{align*}
        \sin_k\left(\theta+\phi\right)
        &= \sin_k{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k{\phi} \\
        \sin_k^\ast\left(\theta+\phi\right)
        &= \sin_k^\ast{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k^\ast{\phi} \\
        \cos_k\left(\theta+\phi\right)
        &= \cos_k{\theta}\cos_k{\phi}-\sign{k}\sin_k{\theta}\sin_k{\phi} \\
        &= \cos_k{\theta}\cos_k{\phi}-\sin_k^\ast{\theta}\sin_k{\phi} \\
        &= \cos_k{\theta}\cos_k{\phi}-\sin_k{\theta}\sin_k^\ast{\phi} \\
    \end{align*}
\end{proposition}
\begin{proof}[\proofof{M:Trigonometry:Sum}]
    Proof by exhaustion.
\end{proof}
\subsection{Matrices}
\begin{definition}\label{M:Rotation}
    \textit{Generalized rotation matrix} is defined as
    \begin{align*}
        R_k\left(\theta\right) & \defeq
        \begin{bmatrix}
            \cos_k{\theta}  & -\sin_k^\ast{\theta} \\
            \sin_k{\theta} & \cos_k{\theta} \\
        \end{bmatrix}\text{,}
    \end{align*}
    where $\theta\in\R$.
\end{definition}
\begin{corollary}[Generalized rotation matrix at zero]\label{M:Rotation:Identity}
\begin{equation*}
    R_k\left(0\right)
    =
    I_2
\end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Rotation:Identity}]
Obvious
\end{proof}
\begin{corollary}[Generalized rotation matrix of sum of arguments]\label{M:Rotation:Sum}
\begin{equation*}
	R_k\left(\theta\right)R_k\left(\phi\right)=R_k\left(\theta+\phi\right)
\end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Rotation:Sum}]
    \begin{align*}
        R_k\left(\theta\right)R_k\left(\phi\right)
        &=\begin{bmatrix}
            \cos_k{\theta}  & -\sin_k^\ast{\theta} \\
            \sin_k{\theta} & \cos_k{\theta} \\
        \end{bmatrix}\begin{bmatrix}
            \cos_k{\phi}  & -\sin_k^\ast{\phi} \\
            \sin_k{\phi} & \cos_k{\phi} \\
        \end{bmatrix} & \text{(\cref{M:Rotation})}\\
        &=\begin{bmatrix}
            \cos_k{\theta}\cos_k{\phi}+\left(-\sin_k^\ast{\theta}\right)\sin_k{\phi} &
            \cos_k{\theta}\left(-\sin_k^\ast{\phi}\right)+\left(-\sin_k^\ast{\theta}\right)\cos_k{\phi} \\
            \sin_k{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k{\phi} &
            \sin_k{\theta}\left(-\sin_k^\ast{\phi}\right)+\cos_k{\theta}\cos_k{\phi} \\
        \end{bmatrix} & \text{(\cref{Matrix:Product})}\\
        &=\begin{bmatrix}
            \cos_k{\theta}\cos_k{\phi}-\sin_k^\ast{\theta}\sin_k{\phi} &
            -\left(\sin_k^\ast{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k^\ast{\phi}\right) \\
            \sin_k{\theta}\cos_k{\phi}+\cos_k{\theta}\sin_k{\phi} &
            \cos_k{\theta}\cos_k{\phi}-\sin_k{\theta}\sin_k^\ast{\phi} \\
        \end{bmatrix} & \text{(simplify)}\\
        &=\begin{bmatrix}
            \cos_k{\theta+\phi}  & -\sin_k^\ast{\theta+\phi} \\
            \sin_k{\theta+\phi} & \cos_k{\theta+\phi} \\
        \end{bmatrix} & \text{(\cref{M:Trigonometry:Sum})} \\
        &= R_k\left(\theta+\phi\right) & \text{(\cref{M:Rotation})} & \qedhere
    \end{align*}
\end{proof}
\begin{corollary}[Inverse of generalized rotation matrix]\label{M:Rotation:Inverse}
\begin{equation*}
	R_k\left(\theta\right)^{-1} = R_k\left(-\theta\right)
\end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Rotation:Inverse}]
    \begin{align*}
        R_k\left(\theta\right)R_k\left(-\theta\right)
        &= R_k\left(0\right) & \text{(\cref{M:Rotation:Sum})} \\
        &= I_2 & \text{(\cref{M:Rotation:Identity})}
    \end{align*}
\end{proof}
\begin{definition}\label{M:Position}
    \textit{Position matrix} is defined recursively as
    \begin{align*}
        P_{k,n}\left(\left\{\tensor{\theta}{^1},\dots,\tensor{\theta}{^n}\right\}\right) & \defeq
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1_{n}},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                                      & 1 \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                                   & {I}_{n-1} \\
        \end{bmatrix}
        T_{2,n+1} \text{,}                                                                                        \\
        P_{k,0}                                                                          & \defeq I_1\text{,}
    \end{align*}
    where $\theta=\left\{\tensor{\theta}{^i}\right\}\in\R^n$ for $i\in\range{1}{n}$.
\end{definition}
\begin{definition}\label{M:Position:Set}
    Let $P\left(n,k\right)$ be set of position matrices.
\end{definition}
\begin{corollary}[Position matrix at zero]\label{M:Position:Set:Identity}
    \begin{equation*}
    	P_{k,n}
    	\left(0_{n}\right)
    	=
    	I_{n+1}
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Position:Set:Identity}]
    Prove by mathematical induction on $n$,
    Let 
    \begin{equation}\label{M:Position:Set:Identity:Proof:Induction}
        P_{k,n-1}\left(0_{n-1}\right)=I_n
    \end{equation}
	\begin{align*}
		P_{k, n}\left(0_{n}\right)
		&=
		\begin{bmatrix}
            P_{k,n-1}\left(0_{n-1}\right) & 0_{n\times 1} \\
            0_{1\times n}                                                                                      & 1 \\
        \end{bmatrix}
        T_{2, n+1}
        \begin{bmatrix}
            R_{k}\left(0\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                                   & {I}_{n-1} \\
        \end{bmatrix}
        T_{2, n+1} & \text{(\cref{M:Position})} \\
		&=
		\begin{bmatrix}
            I_{n} & 0_{n\times 1} \\
            0_{1\times n}                                                                                      & 1 \\
        \end{bmatrix}
        T_{2, n+1}
        \begin{bmatrix}
            R_{k}\left(0\right) & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                                   & {I}_{n-1} \\
        \end{bmatrix}
        T_{2, n+1} & \text{(\cref{M:Position:Set:Identity:Proof:Induction})} \\
		&=
		\begin{bmatrix}
            I_{n} & 0_{n\times 1} \\
            0_{1\times n}                                                                                      & 1 \\
        \end{bmatrix}
        T_{2, n+1}
        \begin{bmatrix}
            I_{2} & 0_{{2}\times{n-1}} \\
            0_{{n-1}\times{2}}                                   & {I}_{n-1} \\
        \end{bmatrix}
        T_{2, n+1} & \text{(\cref{M:Rotation:Identity})} \\
		&=
		I_{n+1}
        T_{2, n+1}
        I_{n+1}
        T_{2, n+1} & \text{(\cref{Matrix:Identity:Block})} \\
		&=
        T_{2, n+1}
        T_{2, n+1} & \text{(\cref{Matrix:Identity})} \\
		&=
        I_{n+1} & \text{(\cref{Matrix:Permutation:Square})}
	\end{align*}
    \begin{equation*}
    	P_{k,n}
    	\left(0_{n}\right)
    	=
    	I_{n+1}
        \qedhere
    \end{equation*}
\end{proof}
\begin{definition}\label{M:Orientation}
    \textit{Orientation matrix} is defined as
    \begin{align*}
        Q^\pm_n\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) & \defeq
        \begin{bmatrix}
            1 & 0_{1\times n}                                                             \\
            0_{n\times 1} & X^\pm_{+1,n-1}\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) \\
        \end{bmatrix}\text{,}                                              \\
        Q^\pm_0                                                & \defeq \pm I_1\text{,}
    \end{align*}
    where $\phi_m\in\R^m \text{for } m\in\range{1}{n-1}$.
\end{definition}
\begin{definition}\label{M:Orientation:Set}
    Let $Q\left(n\right)$ be set of orientation matrices.
\end{definition}
\begin{corollary}[Orientation matrix at zero]\label{M:Orientation:Set:Identity}
    \begin{equation*}
    	Q^{+}_{n}
    	\left(0_{n-1}, 0_{n-2}, \dots\right)
    	=
    	I_{n+1}
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Orientation:Set:Identity}]
    Prove by mathematical induction on $n$,
    Let 
    \begin{equation}\label{M:Orientation:Set:Identity:Proof:Induction}
        Q^+_{n-1}\left(0_{n-2}, 0_{n-3}, \dots\right)=I_n
    \end{equation}
	\begin{align*}
		Q^+_n\left(0_{n-1}, 0_{n-2}, \dots\right)
		&=
		\begin{bmatrix}
            1 & 0_{1\times n}                                                             \\
            0_{n\times 1} & X^\pm_{+1,n-1}\left(0_{n-1},0_{n-2},\dots\right) \\
        \end{bmatrix} & \text{(\cref{M:Orientation})} \\
        &=
		\begin{bmatrix}
            1 & 0_{1\times n}                                                             \\
            0_{n\times 1} & P_{+1,n-1}\left(0_{n-1}\right)Q^+_{n-1}\left(0_{n-2}, 0_{n-3}, \dots\right) \\
        \end{bmatrix} & \text{(\cref{M:Point})} \\
        &=
		\begin{bmatrix}
            1 & 0_{1\times n}                                                             \\
            0_{n\times 1} & P_{+1,n-1}\left(0_{n-1}\right) I_n \\
        \end{bmatrix} & \text{(\cref{M:Orientation:Set:Identity:Proof:Induction})} \\
        &=
		\begin{bmatrix}
            1 & 0_{1\times n}                                                             \\
            0_{n\times 1} & P_{+1,n-1}\left(0_{n-1}\right) \\
        \end{bmatrix} & \text{(\cref{Matrix:Identity})} \\
        &=
		\begin{bmatrix}
            1 & 0_{1\times n}                                                             \\
            0_{n\times 1} & I_{n} \\
        \end{bmatrix} & \text{(\cref{M:Position:Set:Identity})} \\
        &=
		I_{n+1} & \text{(\cref{Matrix:Identity:Block})}
	\end{align*}
    \begin{equation*}
    	Q^+_n
        \left(0_{n-1}, 0_{n-2}, \dots\right)
    	=
    	I_{n+1}
    	\qedhere
    \end{equation*}
\end{proof}
\begin{definition}\label{M:Point}
    \textit{Point matrix} is defined as
    \begin{align*}
        X^\pm_{k,n}\left(\theta,\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) & \defeq
        P_{k,n}\left(\theta\right)
        Q^\pm_n\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right)\text{,}
    \end{align*}
    where $\theta\in\R^n$ and $\phi_m\in\R^m \text{for } m\in\range{1}{n-1}$.
\end{definition}
\begin{definition}\label{M:Point:Set}
    Let $X\left(n,k\right)$ be set of point matrices.
\end{definition}
\begin{corollary}[Position matrix as subset of point matrix]\label{M:Point:Position}
    \begin{equation*}
        P\left(k,n\right)\subset X\left(k,n\right)
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Point:Position}]
    \begin{align*}
    	\forall{P \in P\left(k,n\right)}
    	\forall{Q\in Q\left(n\right)},
    	&
    	P Q \in X\left(k,n\right) & \text{(\cref{M:Point})} \\
    	\implies &
    	P I \in X\left(k,n\right) & \text{(\cref{M:Orientation:Set:Identity})} \\
    	\implies &
    	P \in X\left(k,n\right) & \text{(\cref{Matrix:Identity})} 
    \end{align*}
    \begin{equation*}
    	P\left(k,n\right)\subset X\left(k,n\right) \qedhere
    \end{equation*}
\end{proof}
\begin{corollary}[Point matrix at zero]\label{M:Point:Set:Identity}
    \begin{equation*}
    	X^{+}_{k,n}
    	\left(0_{n}, 0_{n-1}, \dots\right)
    	=
    	I_{n+1}
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{M:Point:Set:Identity}]
	It can be implied from \cref{M:Position:Set:Identity,M:Orientation:Set:Identity}.
\end{proof}
\subsection{Group structure}
Proof of validity of $\group{M}$ and $\subgroup{M}$ (validity of $\groupoperation{\elements}$, proof of the matrices to be $\subgroup{M}$ will later be discussed)...
\begin{lemma}\label{M:Orientation:Group}
    Orientation matrix with multiplication
    is isomorphic to orthogonal group.
\end{lemma}
\begin{proof}[\proofof{M:Orientation:Group}]
    Let
    \begin{APAitemize}
        \item the group of $Q\left(n\right)$ with multiplication is $\left(Q\left(n\right),\cdot\right)$,
        \item the group of $X\left(+1,n\right)$ with multiplication is $\left(X\left(+1,n\right),\cdot\right)$,
        \item the group of $P\left(+1,n\right)$ with multiplication is $\left(P\left(+1,n\right),\cdot\right)$.
    \end{APAitemize}

    Consider the mapping
    \begin{align*}
        f      & : X^{\pm}_{+1,n-1}\left(\phi,\dots\right) \mapsto Q^{\pm}_{n}\left(\phi,\dots\right) & \in X\left(1,n-1\right) \to Q\left(n\right) \text{,} \\
        f^{-1} & : Q^{\pm}_{n}\left(\phi,\dots\right) \mapsto X^{\pm}_{+1,n-1}\left(\phi,\dots\right) & \in Q\left(n\right) \to X\left(1,n-1\right) \text{,}
    \end{align*}
    which by \cref{M:Orientation} is equivalent to
    \begin{equation} \label{M:Point:Orientation:Isomorphism}
        f : X \mapsto \begin{bmatrix}
            1 & 0 \\
            0 & X \\
        \end{bmatrix}
        \text{.}
    \end{equation}

    For any point matrices $X_1, X_2 \in X\left(1,n-1\right)$, it can be shown that
    \begin{align*}
        f\left(X_1\right)\cdot f\left(X_2\right)
                                                 & =\begin{bmatrix}
            1 & 0   \\
            0 & X_1 \\
        \end{bmatrix}\cdot\begin{bmatrix}
            1 & 0   \\
            0 & X_2 \\
        \end{bmatrix}                         & \text{(\cref{M:Point:Orientation:Isomorphism})}\\
                                                 & =\begin{bmatrix}
            1 & 0            \\
            0 & X_1\cdot X_2 \\
        \end{bmatrix}                                                        & \text{(\cref{Matrix:Product:Block})}                                        \\
        f\left(X_1\right)\cdot f\left(X_2\right) & = f\left(X_1\cdot X_2\right)\text{.}
    \end{align*}
    Therefore,
    \begin{align*}
    	\forall n\in\N, 
    	&
    	Q\left(n\right)\cong X\left(+1, n-1\right) \\
    	\implies &
    	Q\left(n\right)\cong \ortho\left(n\right) \iff X\left(+1, n-1\right)\cong \ortho\left(n\right)
    \end{align*}

    \begin{align*}
        X\left(+1, n-1\right)\subset\ortho\left(n\right)
        & \implies
        \forall X\in X\left(+1, n-1\right), X\in\ortho\left(n\right) & \text{()} \\
        & \implies
        \forall X\in X\left(+1, n-1\right), X^{-1}=X^T & \text{()} \\
        & \implies
        \forall Q\in Q\left(n\right)
        \exists X\in X\left(+1, n-1\right), \\
        & Q^{-1}
        =\begin{bmatrix}
        1 & 0 \\
        0 & X \\
        \end{bmatrix}^{-1} \\
        =\begin{bmatrix}
        1 & 0 \\
        0 & X^{-1} \\
        \end{bmatrix} \\
        =\begin{bmatrix}
        1 & 0 \\
        0 & X^{T} \\
        \end{bmatrix} \\
        =\begin{bmatrix}
        1 & 0 \\
        0 & X \\
        \end{bmatrix}^{T} \\
        & \implies
        \forall Q\in Q\left(n\right), Q^{-1}=Q^{T} \\
        & \implies
        \forall Q\in Q\left(n\right), Q\in\ortho\left(n+1\right) \\
        & \implies
        Q\left(n\right)\subset\ortho\left(n+1\right)
    \end{align*}
    \begin{equation}
        X\left(+1, n-1\right)\subset\ortho\left(n\right)
        \implies
        Q\left(n\right)\subset\ortho\left(n+1\right)
    \end{equation}

    \begin{align*}
        T_{i,j}^{-1}=T_{i,j}=T_{i,j}^{T}
        & \implies T_{i,j}\in\ortho \\
        R_1\left(\theta\right)^{-1} = R_1\left(\theta\right)^{T}
        & \implies R_1\left(\theta\right)\in\ortho\left(2\right) \\
        & \implies 
        \begin{bmatrix}
            R_1\left(\theta\right) & 0_{2\times n-1} \\
            0_{n-1\times2} & I_{n-1} \\
        \end{bmatrix}\in\ortho\left(n+1\right)
    \end{align*}
    Prove by mathematical induction on $n$,
    \begin{align*}
        P\left(+1,n-1\right)\subset\ortho\left(n\right)
        & \implies
        \forall P\in P\left(+1,n-1\right),
        P\in\ortho\left(n\right) \\
        & \implies
        \forall P\in P\left(+1,n-1\right),
        \begin{bmatrix}
            P & 0_{n\times1} \\
            0_{1\times n} & 1 \\
        \end{bmatrix}\in\ortho\left(n+1\right) \\
        & \implies
        \begin{bmatrix}
            P_{1,n-1} & 0_{n\times1} \\
            0_{1\times n} & 1 \\
        \end{bmatrix}
        T_{2,n+1}
        \begin{bmatrix}
            R_{1} & 0_{2\times n-1} \\
            0_{n-1\times2} & I_{n-1} \\
        \end{bmatrix}
        T_{2,n+1}
        \in\ortho\left(n+1\right) \\
        & \implies
        \forall P\in P\left(+1,n\right),
        P \in\ortho\left(n+1\right) \\
        & \implies
        P\left(+1,n\right)\subset\ortho\left(n+1\right)
    \end{align*}
    \begin{equation}
        P\left(+1,n\right)\subset\ortho\left(n+1\right)
    \end{equation}
    
    Prove by mathematical induction on $n$,
    \begin{equation}
        X\left(+1, n-1\right)\subset\ortho\left(n\right)
    \end{equation}
    \begin{align*}
    	\forall{X \in X\left(1, n\right)} 
    	&
    	\exists{P \in P\left(1, n\right)}
    	\exists{Q \in Q\left(n\right)},
    	X = P\cdot Q & \text{(\cref{M:Point})} \\
    	\implies
    	&
    	\exists{P \in P\left(1, n\right)}
    	\exists{Q \in \ortho\left(n+1\right)},
    	X = P\cdot Q & \text{()} \\
    	\implies
    	&
    	\exists{P \in \ortho\left(n+1\right)}
    	\exists{Q \in \ortho\left(n\right)},
    	X = P\cdot Q & \text{()} \\
    	\implies
    	&
    	X \in \ortho\left(n+1\right) & \text{()}
    \end{align*}
   	\begin{equation*}
   		X\left(1, n\right) \subset \ortho\left(n+1\right)
   	\end{equation*}
   	
   	\begin{equation}
        \ortho\left(1, n-1\right) \subset X\left(1, n-1\right)
    \end{equation}
    (To be prove)
   	
   	\begin{align*}
   		X\left(1, n-1\right) &= \ortho\left(n\right) \\
   		X\left(1, n-1\right) &\cong \ortho\left(n\right) \\
   		Q\left(n\right) &\cong \ortho\left(n\right)
   	\end{align*}
   	\begin{equation*}
   		Q\left(n\right) \cong \ortho\left(n\right) \qedhere
   	\end{equation*}
\end{proof}
\begin{lemma}\label{M:Point:Group}
    Point matrix with multiplication
    is isomorphic to principal group of the associated Klein geometry.
\end{lemma}
\begin{proof}[\proofof{M:Point:Group}]
    \skipped

    Spherical case is proved successfully with \cref{M:Orientation:Group}.

    Euclidean case should use limits and first degree taylor series.

    Hyperbolic case use \cref{M:Orientation:Group} and closed property.
\end{proof}
\begin{lemma}\label{M:Position:Group}
    Position matrix is isomorphic to quotient group of the associated Klein geometry.
\end{lemma}
\begin{proof}[\proofof{M:Position:Group}]
    \skipped

    It can be seen that $P\cong X/Q \iff X \cong P \ltimes Q$,
    point matrix is isomorphic to the principal group,
    orientation matrix is isomorphic to the subgroup,
    and point matrix is product of position and orientation matrix.

    They may be useful for proving that is not yet to be known.

    Another way is to seek for general form of the matrix element of the group of the Klein geometry.
\end{proof}
\section{Model Parametrization}
\proto{Construction of the charts $\varphi$}
\begin{definition}\label{M:Parameter}
    For point matrix $\tensor{X^\pm_{k,n}\left(\theta,\phi_1,\phi_2,\dots,\phi_n\right)}{}$,
    $n$-dimensional vector $\tensor{\theta}{}$
    is defined as \textit{position parameter}.
\end{definition}
\begin{definition}\label{M:Vector}
    For point matrix $\tensor{X}{}$,
    $\left(n+1\right)$-dimensional column vector $\tensor{p}{}\defeq \frac{1}{k} \tensor{X}{}\cdot\tensor{e}{^1}=\frac{1}{k} \tensor{X}{_1}$
    is defined as \textit{position vector}.
\end{definition}
\begin{definition}\label{M:Vector:Set}
    $P^\ast\left(k,n\right)$ is a set of position vectors.
\end{definition}
\begin{lemma}\label{M:Vector:Position}
    For point matrix $X=PO$
    where $P$ and $O$ are position and orientation matrix respectively,
    $\tensor{p}{}=\frac{1}{k}\tensor{X}{_1}=\frac{1}{k}\tensor{P}{_1}$.
\end{lemma}
\begin{proof}[\proofof{M:Vector:Position}]
    From \cref{M:Orientation}, it is obvious that
    \begin{equation}\label{M:Orientation:Column}
        \tensor{O}{^i_1} =
        \begin{cases}
            1 & \text{if $i=1$,}  \\
            0 & \text{otherwise.} \\
        \end{cases}
    \end{equation}
    \begin{align*}
        \tensor{p}{^i}
         & = \frac{1}{k}\tensor{X}{^i_1}                         &  & \text{From \cref{M:Vector}}       \\
         & = \frac{1}{k}\sum_j{\tensor{P}{^i_j}\tensor{O}{^j_1}} &  & \text{From \cref{Matrix:Product}} \\
         & = \frac{1}{k}\tensor{P}{^i_1}                         &  & \text{From \cref{M:Orientation:Column}}    \\
        \tensor{p}{}
         & = \frac{1}{k}\tensor{X}{_1}                                                                        \\
         & = \frac{1}{k}\tensor{P}{_1}                           &  & \qedhere
    \end{align*}
\end{proof}
\begin{lemma}\label{M:Vector:Value}
    Given position parameter $\tensor{\theta}{}$, position vector can be evaluated as the following.
    \begin{equation*}
        \psi_0^{-1}: \tensor{\theta}{} \mapsto \tensor{p}{}
        = \frac{1}{k}
        \begin{pmatrix}
            \prod_{j\in\Set{1..n}}{\cos_k{\tensor{\theta}{^j}}}                                \\
            \sin_k{\tensor{\theta}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{\theta}{^j}}} \\
            \sin_k{{\tensor{\theta}{^n}}}                                                      \\
        \end{pmatrix}\text{.}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{M:Vector:Value}]
    \skipped

    Use \cref{M:Vector:Position} then $\tensor{p}{}=\frac{1}{k}\tensor{P}{}\cdot\tensor{e}{^1}$ and recursively compute the vector.
\end{proof}
\begin{lemma}\label{PositionParameterValue}
    Given position vector $\tensor{p}{}$, position parameter can be calculated as the following.
    \begin{align*}
        \psi_0
         & : \tensor{p}{} \mapsto \tensor{\theta}{}
        =
        \begin{pmatrix}
            \arcsin_k^{\sign{\tensor{p}{^1}}}{\frac{k\tensor{p}{^2}}{\prod_{j\in\Set{2..n}}{\cos_k{\tensor{\theta}{^j}}}}} \\
            \arcsin_k{\frac{k\tensor{p}{^{i+1}}}{\prod_{j\in\Set{i+1..n}}{\cos_k{\tensor{\theta}{^j}}}}}                   \\
            \arcsin_k{k\tensor{p}{^{n+1}}}                                                                                 \\
        \end{pmatrix}                  \\
         & \in
        \begin{cases}
            P \to \left(-\frac{\pi}{k}, \frac{\pi}{k}\right]\times\left[-\frac{1}{2}\frac{\pi}{k}, \frac{1}{2}\frac{\pi}{k}\right]^{n-1} & \text{if $k>0$}   \\
            P \to \R^{n}                                                                                                              & \text{if $k\le0$} \\
        \end{cases}
    \end{align*}
    where $\cos_k\left(\arcsin_k^{\pm}\left(x\right)\right) = \pm \cos_k\left(\arcsin_k\left(x\right)\right)$.
\end{lemma}
\begin{proof}[\proofof{PositionParameterValue}]
    \skipped

    Use \cref{PositionVectorValue} to compute the inverse mapping.
\end{proof}
\proto{Proof of validity of $\varphi$}
\begin{lemma}\label{CoordinateChart}
    \begin{equation*}
        \Psi=\set{\psi|
            \psi^{-1}
            \in \left(-\frac{1}{2}\frac{\pi}{k},+\frac{1}{2}\frac{\pi}{k}\right)^n \to R
            :\tensor{\theta}{}\mapsto P_{k,n}\left(\tensor{\theta}{}+\tensor{x}{}\right)
            \text{ for }
            \tensor{x}{} \in \R^n
        }
    \end{equation*} is a coordinate chart of a $C^\infty$ differential structure on $R$
\end{lemma}
\begin{proof}[\proofof{CoordinateChart}]
    It is sufficient to shows that
    \begin{APAenumerate}
        \item $R_\psi$ is an open subset of real vector space (defined),
        \item $\bigcup_{\psi\in\Psi} D_\psi = R$ (obvious or not),
        \item transition map is in differentability class $C^\infty$.
    \end{APAenumerate}
    \begin{subproof}{$\bigcup_{\psi\in\Psi} D_\psi = R$}
        \skipped

        Choose $x$ as any then done.
    \end{subproof}
    \begin{subproof}{every transition map is in differentability class $C^\infty$}
        Consider $\psi_1, \psi_2 \in \Psi$ and $x_1, x_2 \in \R^n$ where
        \begin{equation*}
            \psi_i^{-1}
            \in \left(-\frac{1}{2}\frac{\pi}{k},+\frac{1}{2}\frac{\pi}{k}\right)^n \to R
            :\tensor{\theta}{}\mapsto P\left(\tensor{\theta}{}+x_i\right)
            \text{.}
        \end{equation*}

        \skipped

        Use $\psi$ from \cref{PositionVectorMatrix} and \cref{PositionParameterValue}
        and $\psi^{-1}$ from \cref{PositionMatrix}.
        The domain will eventually work itself out and leads to smooth transition mapping.
    \end{subproof}
\end{proof}
\subsection{Locus of position vector}
\begin{lemma}\label{SphericalLocus}
    For $k>0$, $P$ is a $\left(n+1\right)$-sphere of radius $k^{-1}$.
\end{lemma}
\begin{proof}[\proofof{SphericalLocus}]
    \skipped

    Use \cref{TrigonometryPythagorean} and et cetera.
\end{proof}
\begin{lemma}\label{HyperbolicLocus}
    For $k<0$, $P$ is a forward sheet of a two-sheeted $\left(n+1\right)$-hyperboloid of radius $k^{-1}$.
\end{lemma}
\begin{proof}[\proofof{HyperbolicLocus}]
    \skipped

    Use \cref{TrigonometryPythagorean} and et cetera.
\end{proof}
\begin{lemma}\label{EuclideanLocus}
    For $k\to0$, $P$ is a $n$-Euclidean manifold at infinity.
\end{lemma}
\begin{proof}[\proofof{EuclideanLocus}]
    \skipped

    Use \cref{TrigonometryPythagorean} and et cetera.
\end{proof}
\section{Geometric properties}
\proto{Construction of the metric $d$}
\subsection{Embedding}
\begin{definition}\label{Embedding}
    Let $N=\left(Q,g\right)$ be a $n$-dimension Riemannian manifold
    on position parameter space with such inner product $g$ that
    the map $\cdot\mapsto\frac{1}{k}\cdot\times\tensor{e}{^1}\in Q\to P$ is an isometric embedding to $\left(n+1\right)$-Euclidean manifold.
\end{definition}
\begin{lemma}\label{Model:Basis}
    Position parameter $\tensor{\theta}{^i}$ is associated to the following vector in position vector space.
    \begin{equation*}
        \tensor{\theta}{_i} =
        \begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^{i+1}}    \\
            0                                                            \\
        \end{bmatrix}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:Basis}]
    \skipped

    Use \cref{PositionVectorValue} to map from $\tensor{\theta}{}$ to $P$.
    \begin{align*}
        {\tensor{\theta}{_i}}
         & =
        \frac{\partial\tensor{p}{}}{\partial\tensor{\theta}{^i}} \\
         & =
        \begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^{i+1}}    \\
            0                                                            \\
        \end{bmatrix}
    \end{align*}
\end{proof}
\begin{lemma}\label{Model:MetricTensor}
    The metric tensor of $N$ is
    \begin{align*}
        \tensor{g}{_i_j} & =
        \begin{cases}
            \left(\dots\right)\prod_{a>i}{\cos_k^2{\tensor{\theta}{^a}}} & \text{if $i=j$,}  \\
            0                                                            & \text{otherwise.} \\
        \end{cases}
    \end{align*}
\end{lemma}
\begin{proof}[\proofof{Model:MetricTensor}]
    \skipped

    Use \cref{CoordinateChart} to map from $Q$ to $\tensor{\theta}{}$.
    Then use \cref{PositionVectorValue} to map from $\tensor{\theta}{}$ to $P$.
    Then use \cref{Embedding} to find the inner product as multilinear function, result in the metric tensor.

    \begin{align*}
        \tensor{g}{_a_b}\left[\tensor{\theta}{}\right]
         & = \sum_{l,m=1}^{n+1}{
        \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^i}}}
        \tensor{g}{_l_m}\left[\tensor{p}{}\right]
        \frac{\partial{\tensor{p}{^m}}}{\partial{\tensor{\theta}{^j}}}
        }                                                \\
         & = \sum_{l=1}^{n+1}{
        \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^i}}}
        \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^j}}}
        }                                                \\
         & = \tensor{\theta}{_i}\cdot\tensor{\theta}{_j} \\
        \tensor{g}{_a_b}
         & = \begin{cases}
            \tensor{g}{_b_a} & \text{if $a>b$} \\
            \dots            & \text{if $a<b$} \\
            \dots            & \text{if $a=b$}
        \end{cases}
    \end{align*}
    If $a<b$,
    \begin{align*}
        \tensor{g}{_a_b}
         & =\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^{a+1}}    \\
            0                                                            \\
        \end{bmatrix}\cdot\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^b}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^b}\right)\tensor{p}{^{b+1}}    \\
            0                                                            \\
        \end{bmatrix}                                                  \\
         & =
        \sum{k^2\tan_k\left(\tensor{\theta}{^a}\right)\tan_k\left(\tensor{\theta}{^b}\right){\tensor{p}{^j}}^2}
        -\sign{k}k^2\cot_k\left(\tensor{\theta}{^a}\right)\tan_k\left(\tensor{\theta}{^b}\right){\tensor{p}{^{a+1}}}^2 \\
         & =
        \tan_k\left(\tensor{\theta}{^b}\right)
        \tan_k\left(\tensor{\theta}{^a}\right)
        \prod_{a\le j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        -\sign{k}
        \right)                                                                                                        \\
         & =
        \tan_k\left(\tensor{\theta}{^b}\right)
        \tan_k\left(\tensor{\theta}{^a}\right)
        \prod_{a\le j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sign{k}\left(
        1-\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        \right)
        - \sign{k}
        \right)                                                                                                        \\
         & = 0
    \end{align*}
    If $a=b$,
    \begin{align*}
        \tensor{g}{_a_b}
         & =\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^{a+1}}    \\
            0                                                            \\
        \end{bmatrix}\cdot\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^{a+1}}    \\
            0                                                            \\
        \end{bmatrix} \\
         & =
        \sum{\left(k\tensor{p}{^j}\tan_k\tensor{\theta}{^a}\right)^2}
        -\left(k\tensor{p}{^{a+1}}\cot_k\tensor{\theta}{^a}\right)^2  \\
         & =
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum_{1\le i<a}{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        -\cot_k^2\left(\tensor{\theta}{^a}\right)
        \right)
    \end{align*}
\end{proof}
\subsection{Curvature}
\proto{Proof of validity of the metric $d$}
\subsubsection{Curvature (Method I)}
This method may be easier to generalize to
Model II where each direction can have partially independent curvature
(or even Model III where extrinsic curvature become a thing).
But it may be challenging to define Gauss map properly.
\begin{lemma}\label{Model:NormalVector}
    Given a position parameter $\tensor{\theta}{}$,
    the tangent vector in position vector space can be calculated as follows
    \begin{equation*}
        \nu\left(\tensor{\theta}{}\right)
        =
        \begin{cases}
            \begin{bmatrix}
                k\tensor{p}{^1}  \\
                +k\tensor{p}{^i} \\
            \end{bmatrix} & k>0 \\
            \begin{bmatrix}
                1 \\
                0 \\
            \end{bmatrix} & k=0 \\
            \frac{1}{\sqrt{-1+2\left(k\tensor{p}{^1}\right)^2}}
            \begin{bmatrix}
                k\tensor{p}{^1}  \\
                -k\tensor{p}{^i} \\
            \end{bmatrix} & k<0
        \end{cases}
        \text{.}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:NormalVector}]
    \skipped

    Use \cref{SphericalLocus}, \cref{HyperbolicLocus}, \cref{HyperbolicLocus}.

    Nope, use exterior product and hedge operator instead.
    We'll then got
    \begin{align*}
        \tensor{\nu}{}
                    & = \star\bigwedge{\tensor{\theta}{_i}}                                                                        & \text{(need to be normalized)} \\
                    & = \left\vert
        \begin{matrix}
            \tensor{p}{_1}        & \tensor{p}{_2}        & \tensor{p}{_3}        & \dots \\
            \tensor{\theta}{_1^1} & \tensor{\theta}{_1^2} & \tensor{\theta}{_1^3} & \dots \\
            \tensor{\theta}{_2^1} & \tensor{\theta}{_2^2} & \tensor{\theta}{_2^3} & \dots \\
            \dots                 & \dots                 & \dots                 & \dots \\
        \end{matrix}
        \right\vert                                                                                                                                                 \\
                    & = \left\vert
        \begin{matrix}
            \tensor{p}{_1}                                    & \tensor{p}{_2}                                    & \tensor{p}{_3}                             & \dots \\
            -\abs{k}\tan_k{\tensor{\theta}{^1}}\tensor{p}{^1} & k\cot_k{\tensor{\theta}{^1}}\tensor{p}{^2}        & 0                                          & \dots \\
            -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^1} & -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^2} & k\cot_k{\tensor{\theta}{^2}}\tensor{p}{^3} & \dots \\
            \dots                                             & \dots                                             & \dots                                      & \dots \\
        \end{matrix}
        \right\vert                                                                                                                                                 \\
        \tensor{\nu}{^i}
                    & = \left(-1\right)^{i+1}
        \left\vert
        \begin{matrix}
            -\abs{k}\tan_k{\tensor{\theta}{^1}}\tensor{p}{^1} & k\cot_k{\tensor{\theta}{^1}}\tensor{p}{^2}        & 0                                          & \dots \\
            -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^1} & -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^2} & k\cot_k{\tensor{\theta}{^2}}\tensor{p}{^3} & \dots \\
            \dots                                             & \dots                                             & \dots                                      & \dots \\
        \end{matrix}
        \right\vert & \text{(removed $\tensor{p}{^i}$ terms)}                                                                                                       \\
        \tensor{\nu}{^1}
                    & = k^n \prod{\cot_k{\tensor{\theta}{^i}}\tensor{p}{^{i+1}}}                                                                                    \\
        \tensor{\nu}{^2}
                    & = k^n \sign{k} \tan_k{\tensor{\theta}{^1}}\tensor{p}{^2}\prod{\cot_k{\tensor{\theta}{^i}}\tensor{p}{^{i+1}}}                                  \\
        \dots       & = \dots
    \end{align*}
\end{proof}
\begin{lemma}\label{Model:ShapeOperator}
    \begin{equation*}
        S_{P}\left(v\right)
        = \tensor{v}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:ShapeOperator}]
    \skipped

    \begin{align*}
        S_{P}\left(v\right)
                   & = \nabla_v \nu\left(\tensor{P}{}\right)     \\
        \nabla_v f & = v \cdot \frac{\partial f}{\partial x}     \\
        \frac{\partial\nu\left(\tensor{P}{}\right)}{\partial\tensor{\theta}{^i}}
                   & = \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                             \\
        \end{bmatrix}                \\
        S_{\tensor{P}{}}\left(v\right)
                   & = \tensor{v}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{align*}
    false (need to map the derivative to tangent vector space at $\tensor{\theta}{}$)
\end{proof}
\begin{lemma}\label{Model:SecondFundamental}
    \begin{equation*}
        \text{II}_{\tensor{P}{}}\left(v,w\right) = \tensor{v}{^i}\tensor{w}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:SecondFundamental}]
    \skipped

    \begin{align*}
        \text{II}_{\tensor{P}{}}\left(v,w\right)
         & = S_{\tensor{P}{}\left(v\right)} \cdot w                  \\
         & = v \cdot w \cdot \frac{\partial f}{\partial x}           \\
         & = \tensor{v}{^i}\tensor{w}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{align*}
    false (effected)
\end{proof}
\begin{lemma}\label{Model:PrincipalCurvature}

\end{lemma}
\begin{proof}[\proofof{Model:PrincipalCurvature}]
    \skipped

    Eigenvector and Eigenvalue can be solved from the following matrix
    \begin{align*}
        \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                             \\
        \end{bmatrix}
    \end{align*}
    which is an upper triangular matrix.

    Eigenvalue is then the diagonal element
    \begin{align*}
        -\sign{k}\tan_k\left(\tensor{\theta}{^1}\right)\tensor{p}{^1} \\
        -\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^i}\sign{k} \\
        -\cot_k\left(\tensor{\theta}{^n}\right)
    \end{align*}
    false (effected)

    product of any 2 should be the value $\kappa$.
\end{proof}
\subsubsection{Curvature (Method II)}
This method may be easier to be done
(despite the fact that it never finished).
But it raises problems when trying to generalize e.g. dealing with extrinsic curvature
(which may be introduced in Model III
if not to mess with other basis geometries).
\begin{lemma}\label{Model:ChristoffelSymbol}

\end{lemma}
\begin{lemma}\label{Model:RiemannCurvatureTensor}

\end{lemma}
\subsubsection{Curvature (Conclusion)}
\begin{lemma}\label{Model:SectionalCurvature}

\end{lemma}
\proto{Trace back on why $k$ is used all along instead of $\kappa$}
\paragraph{CurvatureParameter}
It can be seen that $\operatorname{sec}\left(p\right) = \kappa = \sign{\left(k\right)}k^2$.
Hence, when provided $\kappa$, $k$ can be determined and used to evaluate the model.
\section{The Model}
\proto{Merge all element back to the model}
\stepcounter{Counter}
\begin{ModelGroupElement}
    For any parameter $\kappa, n$,
    \begin{align*}
        \elements                   & \defeq M                                    \\
        \groupoperation{\elements}  & \defeq \cdot                                \\
        \charts{\elements}          & \defeq \tensor{X}{}\mapsto\tensor{\theta}{} \\
        \innerprod{\elements}       & \defeq g                                    \\
        \points{\elements}          & \equiv R                                    \\
        \transformations{\elements} & \equiv M                                    \\
    \end{align*}
    for injective smooth function $K:\kappa \mapsto k = \sign{\kappa}\sqrt{\abs{\kappa}}\in\R\to\R$.
\end{ModelGroupElement}
\begin{ModelGroupAssertion}
\end{ModelGroupAssertion}
\begin{ModelCurvatureAssertion}
\end{ModelCurvatureAssertion}
\section{Future plan}
\subsection{Model II}
It is very simple to be able to model composite geometries e.g. $S^2 \times E$ by tensor product of the existing model. But to be able to merge them as smooth model may be challenging since not all combination of basis curvature have their own intrinsic geometry. So it may be to find independent variable for each basis or to introduce extrinsic curvature (Model A).
\subsection{Model B}
It is known that $E$ emerged at $n\ge1$ while $S$ and $H$ emerged at $n\ge2$ and there's more complex pure geometries than these that emerged in higher dimension. It is interesting and challenging to explore such geometries and prove whether the curvature still works as indicator in such geometries or are there any patterns for their symmetries.
\subsection{Model A}
This model is based on curvature and mostly just 3 basis geometries and extrinsic curvature which seems to be interesting despite some critical result in some combination e.g. $S^1 \times S^1$ vs $S^2$. It can be even more challenging to have variable curvature with respect to other intrinsic position.
\section*{}
\printbibliography
\begin{figure}
    \centering
    \insertstandalone{figures}{2dplot}
    \caption{Generalized trigonometric functions as function of $k$}\label{TrigonometryPlotted}
    \figurenote{This graph shows the value of generalized trigonometric functions as solid line and trigonometric and hyperbolic functions in the unused domain as dashed line.}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{sine}
    \caption{Generalized sine function}\label{TrigonometrySinePlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{cosine}
    \caption{Generalized cosine function}\label{TrigonometryCosinePlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{tangent}
    \caption{Generalized tangent function}\label{TrigonometryTangentPlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{pointmapping}
    \caption{Matrices-Vectors-Parameters Mapping Diagram}\label{PointMappingDiagram}
\end{figure}
\end{document}

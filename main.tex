%!TEX root = main.tex
%!TEX program = pdflatex
%!TEX spellcheck
\documentclass[stu, babel, american, biblatex, a4paper, draftall]{apa7}
% APA related headers
% https://apastyle.apa.org/style-grammar-guidelines
\usepackage{csquotes}
\addbibresource{lists/prelimitary.bib}
\DeclareLanguageMapping{american}{american-apa}
% ASM related headers (Check APA first)
% https://www.ams.org/publications/authors/AMS-StyleGuide-online.pdf
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{thmtools}
% other headers for specific purpose (Check APA and ASM first)
\usepackage{tensor}
\usepackage{braket}
\usepackage{hyperref}
\usepackage[capitalise, sort&compress, noabbrev, nameinlink]{cleveref}
\iffalse % memory exceeded, include precompiled .pdf instead.
\iffalse % standalone subpreambles invoke errors, manually import instead.
\usepackage[subpreambles=true]{standalone}
\else
\usepackage{standalone}

\usepackage{pgf-umlcd}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

% \usetikzlibrary{external}
% \tikzexternalize
\fi
\usepackage{import}
\newcommand{\insertstandalone}[2]{\import{#1}{#2}}
\else
\newcommand{\insertstandalone}[2]{\includegraphics[width=\textwidth]{#1/#2.pdf}}
\fi

\allowdisplaybreaks
\crefname{paragraph}{paragraph}{paragraphs}
\Crefname{paragraph}{Paragraph}{Paragraphs}
\crefname{subparagraph}{subparagraph}{subparagraphs}
\Crefname{subparagraph}{Subparagraph}{Subparagraphs}


\title{Prove of Constant Radius Model}
\shorttitle{30MA19-02}
\leftheader{Bhunambhon, Nonprasart, Maysamat}  % Author's last names
\authorsnames{Sivakorn Bhunambhon, Hanchai Nonprasart, Sakepisit Maysamat}
\authorsaffiliations{Mahidol Wittayanusorn School}
\course{SCI 30196: Science Project 1}
\professor{Amornsri Amornvatcharapong, Teerapong Suksumran}
\duedate{\today} % Last edit day when the draft watermark is removed.
\abstract{This is the first generation of the model. It's a draft. Who would even care to read the abstract when it's not done anyways?}
\keywords{None 1, None 2, None 3}

\input{lists/notations.def}
\input{lists/theories.def}

\begin{document}
\maketitle
\tableofcontents

\section{Rewrite note}
This proof will be organized using the following guideline
\begin{APAenumerate}
    \item reviewing existing definitions and theorem
    \item proving certain property of existing mathematical objects
    \item defining the conditions of the model
    \item formulating of the model
    \item parametrizing the model
    \item asserting defined property of the model
\end{APAenumerate}
It turned out that figures will crash overleaf, if you want to view such file, please compile them seperately.
To merge the file, it is needed to either pay the subscription or download and run it on your computer.

\section{Prelimitary}
All citation must goes here,
Authors must copy statement they wanted to cite according to the AMS guideline,
Any theorem lemma definitions and others must be in theorem environment if possible (see amsthm user-guide for more details),
Authors then refer to the statement using {\textbackslash}label\{\} and {\textbackslash}cref\{\}.
\subsection{Uncited prelimitary}
\begin{equation}\label{MatrixMultiplication}
    \tensor{C}{}=\tensor{A}{}\tensor{B}{}
    \iff
    \tensor{C}{^i_j}=\sum_k\tensor{A}{^i_k}\tensor{B}{^k_j}
\end{equation}
\begin{definition}\label{KleinGeometry}
    KleinGeometry
\end{definition}
\begin{example}\label{KleinGeometryExample}
    KleinGeometryExamples (I)
\end{example}
\begin{example}\label{ShapeOperator}
    $$S\left(v\right)=\pm\nabla_v n$$
\end{example}
\begin{example}\label{PrincipalCurvature}
    Eigenvalue of second fundamental form (or shape operator)
\end{example}

\section{Objective}
\paragraph{Objective}\label{Objective}
The objective is to construct
\begin{APAenumerate}
    \item a set $\elements$
    \item an algebralic structure (group) on $\elements$ with operation $\groupoperation{\elements}$ (since identity, invertibility, and associativity apply to isometry (curvature-preserving transformations) in the geometry)
    \item an $n$-dimensional $C^\infty$ differential structure on set $\elements$ with chart $\varphi\subset \elements\to\real^n$ (which give it a manifold structure)
    \item an inner product space on $\elements$ with inner product $\innerprod{\elements}\in\elements\times\elements\to\real$ (hence the metric is $d:\left(a,b\right)\mapsto\inner{a^{-1}\groupoperation{\elements}b}{a^{-1}\groupoperation{\elements}b}\in\elements\times\elements\to\real$)
\end{APAenumerate}
(to be determined)
such that
\begin{APAitemize}
    \item $d\left(z\groupoperation{\elements}x,z\groupoperation{\elements}y\right) = d\left(x,y\right)$ for all $x$, $y$ and $z$ in $M$
    \item $d\left(\varphi_i^{-1}\left(x\right), \varphi_i^{-1}\left(y\right)\right)$ is infinitely differentiable by $\kappa$ and all dimension for all $x$ and $y$ in $\real^n$ and $\varphi_i\in\varphi$
    \item all of its two-dimensional linear subspaces have sectional curvature of $\kappa$
\end{APAitemize}
given a natural number $n$ and a real number $\kappa$.

\begin{definition}\label{SetOfPoints}
    \textit{Set of points} $\points{\elements}$ is defined as set of equivalence classes of elements of $\elements$ with the relation $r$,
    where $r\defeq\Set{\left(x,y\right)\in\elements\times\elements|d(x,y)=0}$
    (Generally speaking, distance between those two elements are zero).
\end{definition}
\begin{definition}\label{SetOfTransformations}
    \textit{Set of transformations} $\transformations{\elements}$ is defined as
    $\transformations{\elements}\defeq\Set{f\in\elements\to\elements|\exists y \in\elements, f\left(x\right) = y \groupoperation{\elements} x}$.
\end{definition}
\begin{definition}\label{ModelGroup}
    \textit{Principal group} and \textit{subgroup} of the model is defined as
    \begin{APAitemize}
        \item Let $\group{\elements}$ be a group on set $\elements$ together with binary operation $\groupoperation{\elements}$.
        \item Let $\subgroup{\elements}$ be a group on set $P\in\points{\elements}$ together with binary operation $\groupoperation{\elements}$.
    \end{APAitemize}
    respectively.
\end{definition}
\begin{conjecture}\label{GeometricGroupStructure}
    If parameters $\left(\kappa,n\right)$ is associated with Klein geometry $\left(G,H\right)$
    then $\group{\elements}\cong G$ and $\subgroup{\elements}\cong H$.

    From \cref{KleinGeometryExample},
    \begin{APAitemize}
        \item For $\kappa>0$, $\group{\elements}\cong\ortho\left(n+1\right)$ and $\subgroup{\elements}\cong\ortho\left(n\right)$.
        \item For $\kappa=0$, $\group{\elements}\cong\euclid\left(n\right)$ and $\subgroup{\elements}\cong\ortho\left(n\right)$.
        \item For $\kappa<0$, $\group{\elements}\cong\ortho^{+}\left(1,n\right)$ and $\subgroup{\elements}\cong\ortho\left(n\right)$.
    \end{APAitemize}
\end{conjecture}
\section{Model Foundation}
\proto{Construction of the set $M$ and operator $\groupoperation{\elements}$}
\subsection{Trigonometry}
\begin{definition}\label{Trigonometry}
    \textit{Generalized trigonometric functions} $f_k:\mathbb{R}\to\mathbb{R}$ are defined as
    \begin{align*}
        f_k\left(\theta\right) & \defeq
        \begin{cases}
            g\left(k\theta\right) & \text{if $k\geq0$,} \\
            h\left(k\theta\right) & \text{otherwise,}   \\
        \end{cases}
    \end{align*}
    where $g$ (resp. $h$) are the associated trigonometric (resp. hyperbolic) function.
\end{definition}
\begin{example}\label{TrigonometrySine}
    \textit{Generalized sine function} (see \cref{TrigonometrySinePlotted}) is defined as
    \begin{align*}
        \sin_k{\theta} & \defeq
        \begin{cases}
            \sin\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \sinh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}
    \end{align*}
\end{example}
\begin{example}\label{TrigonometryCosine}
    \textit{Generalized cosine function} (see \cref{TrigonometryCosinePlotted}) is defined as
    \begin{align*}
        \cos_k{\theta} & \defeq
        \begin{cases}
            \cos\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \cosh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}
    \end{align*}
\end{example}
\begin{example}\label{TrigonometryTangent}
    \textit{Generalized tangent function} (see \cref{TrigonometryTangentPlotted}) is defined as
    \begin{align*}
        \tan_k{\theta} & \defeq
        \begin{cases}
            \tan\left(k\theta\right)  & \text{if $k\geq0$,} \\
            \tanh\left(k\theta\right) & \text{otherwise.}   \\
        \end{cases}
    \end{align*}
\end{example}
\begin{corollary}\label{TrigonometryContinuity}
    Generalized sine and cosine function are infinitely differentiable.
\end{corollary}
\begin{proof}[\proofof{TrigonometryContinuity}]
    % Since $\sin$, $\cos$, $\sinh$, and $\cosh$ are infinitely differentiable,
    % $\sin_k$ and $\cos_k$ are infinitely differentiable by $\theta$
    % and $\sin_k$ and $\cos_k$ are infinitely differentiable by $k$ at $k\ne0$.

    % \begin{align*}
    %     \frac{d}{dk}\sin_k^{\prime}\theta\left(0\right)
    %      & = \lim_{h\to0}\frac{\sin_{h}\theta-\sin_{-h}\theta}{2h}                          \\
    %      & = \lim_{h\to0}\frac{\sin{\left(h\right)\theta}-\sinh{\left(-h\right)\theta}}{2h} \\
    %      & = \lim_{h\to0}\frac{\left(h\right)\theta-\left(-h\right)\theta}{2h}              \\
    %      & = \lim_{h\to0}\theta                                                             \\
    %      & = \theta
    % \end{align*}

    % Since $\sin^{\prime}\theta=\cos\theta$ and $\sinh^{\prime}\theta=\cosh\theta$,
    % \begin{align*}
    %     \sin_k^{\prime}\theta
    %      & =
    %     \begin{cases}
    %         \theta\cos{k\theta}  & \text{if $k>0$,} \\
    %         \theta               & \text{if $k=0$,} \\
    %         \theta\cosh{k\theta} & \text{if $k<0$.} \\
    %     \end{cases} \\
    %      & = \theta\cos_k{\theta}
    % \end{align*}

    % Similarly, $\cos_k^{\prime}=-\sign{\left(k\right)}\theta\sin_k{\theta}$.

    \skipped

    By using mathematical induction, it can be proved that
    \begin{align*}
        \sin_k^{\left(4n\right)}\theta   & = \theta^{4n}\sin_k{\theta}                         \\
        \cos_k^{\left(4n\right)}\theta   & = \theta^{4n}\cos_k{\theta}                         \\
        \sin_k^{\left(4n+1\right)}\theta & = \theta^{4n+1}\cos_k{\theta}                       \\
        \cos_k^{\left(4n+1\right)}\theta & = -\sign{\left(k\right)}\theta^{4n+1}\sin_k{\theta} \\
        \sin_k^{\left(4n+2\right)}\theta & = -\sign{\left(k\right)}\theta^{4n+2}\sin_k{\theta} \\
        \cos_k^{\left(4n+2\right)}\theta & = -\sign{\left(k\right)}\theta^{4n+2}\cos_k{\theta} \\
        \sin_k^{\left(4n+3\right)}\theta & = -\sign{\left(k\right)}\theta^{4n+3}\cos_k{\theta} \\
        \cos_k^{\left(4n+3\right)}\theta & = \theta^{4n+3}\sin_k{\theta}
    \end{align*}
    Hence, generalized sine and cosine function can be infinitely differentiable.
\end{proof}
\begin{theorem}[Pythagorean's identity equivalence]\label{TrigonometryPythagorean}
    \begin{align*}
        \cos_k^2{\theta} & +\sign{k}\sin_k^2{\theta} & = 1                \\
        1                & +\sign{k}\tan_k^2{\theta} & = \sec_k^2{\theta} \\
        \cot_k^2{\theta} & +\sign{k}                 & = \csc_k^2{\theta}
    \end{align*}
\end{theorem}
\begin{proof}[\proofof{TrigonometryPythagorean}]
    \skipped

    Proof by exhaustion. (Proof by cases.)
\end{proof}
\subsection{Matrices}
\begin{definition}\label{RotationMatrix}
    \textit{Generalized rotation matrix} is defined as
    \begin{align*}
        R_k\left(\theta\right) & \defeq
        \begin{bmatrix}
            \cos_k{\theta}  & \sin_k{\theta} \\
            -\sin_k{\theta} & \cos_k{\theta} \\
        \end{bmatrix}\text{,}
    \end{align*}
    where $\theta\in\real$.
\end{definition}
\begin{definition}\label{PositionMatrix}
    \textit{Position matrix} is defined recursively as
    \begin{align*}
        P_{k,n}\left(\left\{\tensor{\theta}{^1},\dots,\tensor{\theta}{^n}\right\}\right) & \defeq
        \begin{bmatrix}
            P_{k,n-1}\left(\left\{\tensor{\theta}{^1},\dots,\tensor{\theta}{^{n-1}}\right\}\right) & 0 \\
            0                                                                                      & 1 \\
        \end{bmatrix}
        T_\pi
        \begin{bmatrix}
            R_k\left(\tensor{\theta}{^n}\right) & 0 \\
            0                                   & I \\
        \end{bmatrix}
        T_\pi \text{,}                                                                                        \\
        P_{k,0}                                                                          & \defeq I_1\text{,}
    \end{align*}
    where $\pi=\begin{pmatrix}
            1 & 2   & 3 & \dots & n & n+1 \\
            1 & n+1 & 3 & \dots & n & 2   \\
        \end{pmatrix}$
    and $\theta=\left\{\tensor{\theta}{^i}\right\}\in\real^n  \text{for } i\in\range{1}{n}$.
\end{definition}
\begin{definition}\label{SetPositionMatrix}
    Let $R\left(n,k\right)$ be set of point matrices.
\end{definition}
\begin{definition}\label{OrientationMatrix}
    \textit{Orientation matrix} is defined as
    \begin{align*}
        O^\pm_n\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) & \defeq
        \begin{bmatrix}
            1 & 0                                                             \\
            0 & X^\pm_{+1,n-1}\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) \\
        \end{bmatrix}\text{,}                                              \\
        O^\pm_0                                                & \defeq \pm I_1\text{,}
    \end{align*}
    where $\phi_m\in\real^m \text{for } m\in\range{1}{n-1}$.
\end{definition}
\begin{definition}\label{PointMatrix}
    \textit{Point matrix} is defined as
    \begin{align*}
        X^\pm_{k,n}\left(\theta,\phi_{n-1},\phi_{n-2},\dots,\phi_1\right) & \defeq
        P_{k,n}\left(\theta\right)
        O^\pm_n\left(\phi_{n-1},\phi_{n-2},\dots,\phi_1\right)\text{,}
    \end{align*}
    where $\theta\in\real^n$ and $\phi_m\in\real^m \text{for } m\in\range{1}{n-1}$.
\end{definition}
\begin{definition}\label{SetPointMatrix}
    Let $M\left(n,k\right)$ be set of point matrices.
\end{definition}
\begin{corollary}\label{PointPositionSubset}
    \begin{equation*}
        P\left(k,n\right)\subset M\left(k,n\right)
    \end{equation*}
\end{corollary}
\begin{proof}[\proofof{PointPositionSubset}]
    \skipped

    $I$ is an orientation matrix for some $\psi$.
\end{proof}
\subsection{Group structure}
Proof of validity of $\group{M}$ and $\subgroup{M}$ (validity of $\groupoperation{\elements}$, proof of the matrices to be $\subgroup{M}$ will later be discussed)...
\begin{lemma}\label{OrientationGroup}
    Orientation matrix with multiplication
    is isomorphic to orthogonal group.
\end{lemma}
\begin{proof}[\proofof{OrientationGroup}]
    Let
    \begin{APAitemize}
        \item the group of orientation matrix $O^{\pm}_{n}$ with multiplication is $\left(Q\left(n\right),\cdot\right)$,
        \item the group of point matrix $X^{\pm}_{+1,n-1}$ at $k=+1$ with multiplication is $\left(M\left(n\right),\cdot\right)$.
    \end{APAitemize}

    \begin{subproof}{$Q\left(n\right)\cong M\left(n-1\right)$}
        Consider the mapping
        \begin{align*}
            f      & : X^{\pm}_{+1,n-1}\left(\phi,\dots\right) \mapsto O^{\pm}_{n}\left(\phi,\dots\right) & \in M\left(n-1\right) \to Q\left(n\right) \text{,} \\
            f^{-1} & : O^{\pm}_{n}\left(\phi,\dots\right) \mapsto X^{\pm}_{+1,n-1}\left(\phi,\dots\right) & \in Q\left(n\right) \to M\left(n-1\right) \text{,}
        \end{align*}
        which by \cref{OrientationMatrix} is equivalent to
        \begin{equation} \label{PointToOrientationIsomorphism}
            f : X \mapsto \begin{bmatrix}
                1 & 0 \\
                0 & X \\
            \end{bmatrix}
            \text{.}
        \end{equation}

        For any point matrices $X_1, X_2 \in M\left(n-1\right)$, it can be shown that
        \begin{align}
            f\left(X_1\right)\cdot f\left(X_2\right)
                                                     & =\begin{bmatrix}
                1 & 0   \\
                0 & X_1 \\
            \end{bmatrix}\cdot\begin{bmatrix}
                1 & 0   \\
                0 & X_2 \\
            \end{bmatrix}                         &  & \text{(substitute \cref{PointToOrientationIsomorphism})} \nonumber \\
                                                     & =\begin{bmatrix}
                1 & 0            \\
                0 & X_1\cdot X_2 \\
            \end{bmatrix}                                                        &  & \text{(simplify)} \nonumber                                        \\
            f\left(X_1\right)\cdot f\left(X_2\right) & = f\left(X_1\cdot X_2\right)\text{.} \label{PointToOrientationIsomorphismProperty}
        \end{align}
        Therefore, $Q\left(n\right)\cong M\left(n-1\right)$.
    \end{subproof}

    Hence, it is sufficient to shows that $M\left(n-1\right)\cong O\left(n\right)$.

    To show that the isomorphism is the identity mapping
    (to shows that $M\left(n-1\right)=O\left(n\right)$)
    is sufficient to shows that
    \begin{APAenumerate}
        \item Every points matrices $X \in M\left(n-1\right)$ is an element of orthogonal group $\ortho\left(n\right)$.
        \item Every orthogonal matrices $A \in \ortho\left(n\right)$ is an point matrix in $M\left(n-1\right)$.
    \end{APAenumerate}
    \begin{subproof}{$\forall n\in\nat, \forall X\in M\left(n-1\right), X\in \ortho\left(n\right)$}
        \begin{align*}
            \forall n\in\nat, \forall X\in M\left(n-1\right),
                 & X\in \ortho\left(n\right)                                  \\
            \iff & X\in \set{A\in\genlin\left(n\right)\mid X X^T = X^T X = I} \\
            \iff & X\in\genlin\left(n\right) \wedge X X^T = X^T X = I
        \end{align*}

        It is obvious that $X$ is an $n$-by-$n$ square matrix.

        And if $X$ is orthogonal ($X X^T = X^T X = I$),
        then there exist an $n$-by-$n$ square matrix $X^{-1} = X^T$ such that $X X^{-1} = Y X^{-1} = T$ ($X$ is invertible).
        Thus, $X\in\genlin\left(n\right)$.

        Hence, it is sufficient to shows that $X$ is orthogonal.

        \begin{subproof}{$X X^T = X^T X = I$}
            To show that $X$ is orthogonal.

            \skipped

            Use closed property of orthogonal group.

            It can be proved that position matrix with such parameter (or its factor) is orthogonal.

            It can also be prove inductively that orthogonal matrix is orthogonal by using its block form and base case of point matrix.
        \end{subproof}
    \end{subproof}
    \begin{subproof}{$\forall n\in\nat, \forall A\in \ortho\left(n-1\right), \exists X\in M\left(n-1\right), A=X$}
        \skipped

        No idea on how to prove yet.
    \end{subproof}
\end{proof}
\begin{lemma}\label{PointGroup}
    Point matrix with multiplication
    is isomorphic to principal group of the associated Klein geometry.
\end{lemma}
\begin{proof}[\proofof{PointGroup}]
    \skipped

    Spherical case is proved successfully with \cref{OrientationGroup}.

    Euclidean case should use limits and first degree taylor series.

    hyperbolic case is similar to \cref{OrientationGroup} such that it use closed property.
\end{proof}
\begin{lemma}\label{PositionGroup}
    Position matrix is isomorphic to quotient group of the associated Klein geometry.
\end{lemma}
\begin{proof}[\proofof{PositionGroup}]
    \skipped

    It can be seen that $Q\cong M/P \iff M \cong Q \ltimes P$,
    point matrix is isomorphic to the principal group,
    orientation matrix is isomorphic to the subgroup,
    and point matrix is product of position and orientation matrix.

    They may be useful for proving that is not yet to be known.

    Another way is to seek for general form of the matrix element of the group of the Klein geometry.
\end{proof}
\section{Model Parametrization}
\proto{Construction of the charts $\varphi$}
\begin{definition}\label{PositionParameter}
    For point matrix $\tensor{X^\pm_{k,n}\left(\theta,\phi_1,\phi_2,\dots,\phi_n\right)}{}$,
    $n$-dimensional vector $\tensor{\theta}{}$
    is defined as \textit{position parameter}.
\end{definition}
\begin{definition}\label{PositionVector}
    For point matrix $\tensor{X}{}$,
    $\left(n+1\right)$-dimensional column vector $\tensor{p}{}\defeq \frac{1}{k} \tensor{X}{}\cdot\tensor{e}{^1}=\frac{1}{k} \tensor{X}{_1}$
    is defined as \textit{position vector}.
\end{definition}
\begin{definition}\label{PositionVectorSet}
    $P\left(k,n\right)$ is a set of position vectors.
\end{definition}
\proto{This will provide a hint on why the matrices are $\subgroup{M}$...}
\begin{lemma}\label{PositionVectorMatrix}
    For point matrix $X=PO$
    where $P$ and $O$ are position and orientation matrix respectively,
    $\tensor{p}{}=\frac{1}{k}\tensor{X}{_1}=\frac{1}{k}\tensor{P}{_1}$.
\end{lemma}
\begin{proof}[\proofof{PositionMatrix}]
    From \cref{OrientationMatrix}, it is obvious that
    \begin{equation}\label{OrientationColumn}
        \tensor{O}{^i_1} =
        \begin{cases}
            1 & \text{if $i=1$,}  \\
            1 & \text{otherwise.} \\
        \end{cases}
    \end{equation}
    \begin{align*}
        \tensor{p}{^i}
         & = \frac{1}{k}\tensor{X}{^i_1}                         &  & \text{From \cref{PositionVector}}       \\
         & = \frac{1}{k}\sum_j{\tensor{P}{^i_j}\tensor{O}{^j_1}} &  & \text{From \cref{MatrixMultiplication}} \\
         & = \frac{1}{k}\tensor{P}{^i_1}                         &  & \text{From \cref{OrientationColumn}}    \\
        \tensor{p}{}
         & = \frac{1}{k}\tensor{X}{_1}                                                                        \\
         & = \frac{1}{k}\tensor{P}{_1}                           &  & \qedhere
    \end{align*}
\end{proof}
\begin{lemma}\label{PositionVectorValue}
    Given position parameter $\tensor{\theta}{}$, position vector can be evaluated as the following.
    \begin{equation*}
        \psi_0^{-1}: \tensor{\theta}{} \mapsto \tensor{p}{}
        = \frac{1}{k}
        \begin{pmatrix}
            \prod_{j\in\Set{1..n}}{\cos_k{\tensor{\theta}{^j}}}                                \\
            \sin_k{\tensor{\theta}{^{i-1}}}\prod_{j\in\Set{i..n}}{\cos_k{\tensor{\theta}{^j}}} \\
            \sin_k{{\tensor{\theta}{^n}}}                                                      \\
        \end{pmatrix}\text{.}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{PositionVectorValue}]
    \skipped

    Use \cref{PositionVectorMatrix} then $\tensor{p}{}=\frac{1}{k}\tensor{P}{}\cdot\tensor{e}{^1}$ and recursively compute the vector.
\end{proof}
\begin{lemma}\label{PositionParameterValue}
    Given position vector $\tensor{p}{}$, position parameter can be calculated as the following.
    \begin{align*}
        \psi_0
         & : \tensor{p}{} \mapsto \tensor{\theta}{}
        =
        \begin{pmatrix}
            \arcsin_k^{\sign{\tensor{p}{^1}}}{\frac{k\tensor{p}{^2}}{\prod_{j\in\Set{2..n}}{\cos_k{\tensor{\theta}{^j}}}}} \\
            \arcsin_k{\frac{k\tensor{p}{^{i+1}}}{\prod_{j\in\Set{i+1..n}}{\cos_k{\tensor{\theta}{^j}}}}}                   \\
            \arcsin_k{k\tensor{p}{^{n+1}}}                                                                                 \\
        \end{pmatrix}                  \\
         & \in
        \begin{cases}
            P \to \left(-\frac{\pi}{k}, \frac{\pi}{k}\right]\times\left[-\frac{1}{2}\frac{\pi}{k}, \frac{1}{2}\frac{\pi}{k}\right]^{n-1} & \text{if $k>0$}   \\
            P \to \real^{n}                                                                                                              & \text{if $k\le0$} \\
        \end{cases}
    \end{align*}
    where $\cos_k\left(\arcsin_k^{\pm}\left(x\right)\right) = \pm \cos_k\left(\arcsin_k\left(x\right)\right)$.
\end{lemma}
\begin{proof}[\proofof{PositionParameterValue}]
    \skipped

    Use \cref{PositionVectorValue} to compute the inverse mapping.
\end{proof}
\proto{Proof of validity of $\varphi$}
\begin{lemma}\label{CoordinateChart}
    \begin{equation*}
        \Psi=\set{\psi|
            \psi^{-1}
            \in \left(-\frac{1}{2}\frac{\pi}{k},+\frac{1}{2}\frac{\pi}{k}\right)^n \to R
            :\tensor{\theta}{}\mapsto P_{k,n}\left(\tensor{\theta}{}+\tensor{x}{}\right)
            \text{ for }
            \tensor{x}{} \in \real^n
        }
    \end{equation*} is a coordinate chart of a $C^\infty$ differential structure on $R$
\end{lemma}
\begin{proof}[\proofof{CoordinateChart}]
    It is sufficient to shows that
    \begin{APAenumerate}
        \item $R_\psi$ is an open subset of real vector space (defined),
        \item $\bigcup_{\psi\in\Psi} D_\psi = R$ (obvious or not),
        \item transition map is in differentability class $C^\infty$.
    \end{APAenumerate}
    \begin{subproof}{$\bigcup_{\psi\in\Psi} D_\psi = R$}
        \skipped

        Choose $x$ as any then done.
    \end{subproof}
    \begin{subproof}{every transition map is in differentability class $C^\infty$}
        Consider $\psi_1, \psi_2 \in \Psi$ and $x_1, x_2 \in \real^n$ where
        \begin{equation*}
            \psi_i^{-1}
            \in \left(-\frac{1}{2}\frac{\pi}{k},+\frac{1}{2}\frac{\pi}{k}\right)^n \to R
            :\tensor{\theta}{}\mapsto P\left(\tensor{\theta}{}+x_i\right)
            \text{.}
        \end{equation*}

        \skipped

        Use $\psi$ from \cref{PositionVectorMatrix} and \cref{PositionParameterValue}
        and $\psi^{-1}$ from \cref{PositionMatrix}.
        The domain will eventually work itself out and leads to smooth transition mapping.
    \end{subproof}
\end{proof}
\subsection{Locus of position vector}
\begin{lemma}\label{SphericalLocus}
    For $k>0$, $P$ is a $\left(n+1\right)$-sphere of radius $k^{-1}$.
\end{lemma}
\begin{proof}[\proofof{SphericalLocus}]
    \skipped

    Use \cref{TrigonometryPythagorean} and et cetera.
\end{proof}
\begin{lemma}\label{HyperbolicLocus}
    For $k<0$, $P$ is a forward sheet of a two-sheeted $\left(n+1\right)$-hyperboloid of radius $k^{-1}$.
\end{lemma}
\begin{proof}[\proofof{HyperbolicLocus}]
    \skipped

    Use \cref{TrigonometryPythagorean} and et cetera.
\end{proof}
\begin{lemma}\label{EuclideanLocus}
    For $k\to0$, $P$ is a $n$-Euclidean manifold at infinity.
\end{lemma}
\begin{proof}[\proofof{EuclideanLocus}]
    \skipped

    Use \cref{TrigonometryPythagorean} and et cetera.
\end{proof}
\section{Geometric properties}
\proto{Construction of the metric $d$}
\subsection{Embedding}
\begin{definition}\label{Embedding}
    Let $N=\left(Q,g\right)$ be a $n$-dimension Riemannian manifold
    on position parameter space with such inner product $g$ that
    the map $\cdot\mapsto\frac{1}{k}\cdot\times\tensor{e}{^1}\in Q\to P$ is an isometric embedding to $\left(n+1\right)$-Euclidean manifold.
\end{definition}
\begin{lemma}\label{Model:Basis}
    Position parameter $\tensor{\theta}{^i}$ is associated to the following vector in position vector space.
    \begin{equation*}
        \tensor{\theta}{_i} =
        \begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^{i+1}}          \\
            0                                           \\
        \end{bmatrix}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:Basis}]
    \skipped

    Use \cref{PositionVectorValue} to map from $\tensor{\theta}{}$ to $P$.
    \begin{align*}
        {\tensor{\theta}{_i}}
         & =
        \frac{\partial\tensor{p}{}}{\partial\tensor{\theta}{^i}} \\
         & =
        \begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^{i+1}}          \\
            0                                           \\
        \end{bmatrix}
    \end{align*}
\end{proof}
\begin{lemma}\label{Model:MetricTensor}
    The metric tensor of $N$ is
    \begin{align*}
        \tensor{g}{_i_j} & =
        \begin{cases}
            \left(
            1 - \sign{k}
            - \cos_k^2\left(\tensor{\theta}{^a}\right)
            - \sin_k^2\left(\tensor{\theta}{^b}\right)
        \right)\prod_{a>i}{\cos_k^2{\tensor{\theta}{^a}}} & \text{if $i=j$,}  \\
            0                                          & \text{otherwise.} \\
        \end{cases}
    \end{align*}
\end{lemma}
\begin{proof}[\proofof{Model:MetricTensor}]
    \skipped

    Use \cref{CoordinateChart} to map from $Q$ to $\tensor{\theta}{}$.
    Then use \cref{PositionVectorValue} to map from $\tensor{\theta}{}$ to $P$.
    Then use \cref{Embedding} to find the inner product as multilinear function, result in the metric tensor.
    
    \begin{align*}
        \tensor{g}{_a_b}\left[\tensor{\theta}{}\right]
        & = \sum_{l,m=1}^{n+1}{
            \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^i}}}
            \tensor{g}{_l_m}\left[\tensor{p}{}\right]
            \frac{\partial{\tensor{p}{^m}}}{\partial{\tensor{\theta}{^j}}}
        } \\
        & = \sum_{l=1}^{n+1}{
            \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^i}}}
            \frac{\partial{\tensor{p}{^l}}}{\partial{\tensor{\theta}{^j}}}
        } \\
        & = \tensor{\theta}{_i}\cdot\tensor{\theta}{_j} \\
        \tensor{g}{_a_b}
        & = \begin{cases}
            \tensor{g}{_b_a} & \text{if $a>b$} \\
            \dots & \text{if $a<b$} \\
            \dots & \text{if $a=b$}
        \end{cases}
    \end{align*}
    If $a<b$,
    \begin{align*}
        \tensor{g}{_a_b}
        &=\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^{a+1}}          \\
            0                                           \\
        \end{bmatrix}\cdot\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^b}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^b}\right)\tensor{p}{^{b+1}}          \\
            0                                           \\
        \end{bmatrix} \\
        &=
        \sum{k^2\tan_k\left(\tensor{\theta}{^a}\right)\tan_k\left(\tensor{\theta}{^b}\right){\tensor{p}{^j}}^2}
        -\sign{k}k^2\cot_k\left(\tensor{\theta}{^a}\right)\tan_k\left(\tensor{\theta}{^b}\right){\tensor{p}{^{a+1}}}^2 \\
        &=
        \tan_k\left(\tensor{\theta}{^b}\right)
        \tan_k\left(\tensor{\theta}{^a}\right)
        \prod_{a\le j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        -\sign{k}
        \right)\\
        &=
        \tan_k\left(\tensor{\theta}{^b}\right)
        \tan_k\left(\tensor{\theta}{^a}\right)
        \prod_{a\le j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sign{k}\left(
            1-\prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        \right)
        - \sign{k}
        \right)\\
        &= 0
    \end{align*}
    If $a=b$,
    \begin{align*}
        \tensor{g}{_a_b}
        &=\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^{a+1}}          \\
            0                                           \\
        \end{bmatrix}\cdot\begin{bmatrix}
            -\abs{k}\tan_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^j} \\
            k\cot_k\left(\tensor{\theta}{^a}\right)\tensor{p}{^{a+1}}          \\
            0                                           \\
        \end{bmatrix} \\
        &=
        \sum{\left(k\tensor{p}{^j}\tan_k\tensor{\theta}{^a}\right)^2}
        -\sign{k}\left(k\tensor{p}{^{a+1}}\cot_k\tensor{\theta}{^a}\right)^2 \\
        &=
        \sin_k^2\left(\tensor{\theta}{^a}\right)
        \prod_{a<j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \left(
        \prod_{1\le j<a}{\cos_k^2\tensor{\theta}{^j}}
        + \sum{
            \sin_k^2{\tensor{\theta}{^{i}}}\prod_{i<j<a}{\cos_k^2\tensor{\theta}{^j}}
        }
        -\sign{k}\cot_k^2\left(\tensor{\theta}{^a}\right)
        \right)\\
        &=
        \sin_k^2\left(\tensor{\theta}{^1}\right)
        \prod_{a<j\le n+1}{\cos_k^2\tensor{\theta}{^j}}
        \sign{k}
        \left(
            1 - \sign{k} - \sin_k^{-2}\left(\tensor{\theta}{^a}\right)
        \right) \\
        &=
        \left(
            1 - \sign{k}
            - \cos_k^2\left(\tensor{\theta}{^a}\right)
            - \sin_k^2\left(\tensor{\theta}{^b}\right)
        \right)
        \prod_{a<j}{\cos_k^2\tensor{\theta}{^j}}
    \end{align*}
\end{proof}
\subsection{Curvature}
\proto{Proof of validity of the metric $d$}
\subsubsection{Curvature (Method I)}
This method may be easier to generalize to
Model II where each direction can have partially independent curvature
(or even Model III where extrinsic curvature become a thing).
But it may be challenging to define Gauss map properly.
\begin{lemma}\label{Model:NormalVector}
    Given a position parameter $\tensor{\theta}{}$,
    the tangent vector in position vector space can be calculated as follows
    \begin{equation*}
        \nu\left(\tensor{\theta}{}\right)
        =
        \begin{cases}
            \begin{bmatrix}
                k\tensor{p}{^1}  \\
                +k\tensor{p}{^i} \\
            \end{bmatrix} & k>0 \\
            \begin{bmatrix}
                1 \\
                0 \\
            \end{bmatrix} & k=0 \\
            \frac{1}{\sqrt{-1+2\left(k\tensor{p}{^1}\right)^2}}
            \begin{bmatrix}
                k\tensor{p}{^1}  \\
                -k\tensor{p}{^i} \\
            \end{bmatrix} & k<0
        \end{cases}
        \text{.}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:NormalVector}]
    \skipped

    Use \cref{SphericalLocus}, \cref{HyperbolicLocus}, \cref{HyperbolicLocus}.

    Nope, use exterior product and hedge operator instead.
    We'll then got
    \begin{align*}
        \tensor{\nu}{}
        &= \star\bigwedge{\tensor{\theta}{_i}} & \text{(need to be normalized)}\\
        &= \left\vert
        \begin{matrix}
        \tensor{p}{_1} & \tensor{p}{_2} & \tensor{p}{_3} & \dots \\
        \tensor{\theta}{_1^1} & \tensor{\theta}{_1^2} & \tensor{\theta}{_1^3} & \dots \\
        \tensor{\theta}{_2^1} & \tensor{\theta}{_2^2} & \tensor{\theta}{_2^3} & \dots \\
        \dots & \dots & \dots & \dots \\
        \end{matrix}
        \right\vert \\
        &= \left\vert
        \begin{matrix}
        \tensor{p}{_1} & \tensor{p}{_2} & \tensor{p}{_3} & \dots \\
        -\abs{k}\tan_k{\tensor{\theta}{^1}}\tensor{p}{^1} & k\cot_k{\tensor{\theta}{^1}}\tensor{p}{^2} & 0 & \dots \\
        -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^1} & -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^2} & k\cot_k{\tensor{\theta}{^2}}\tensor{p}{^3} & \dots \\
        \dots & \dots & \dots & \dots \\
        \end{matrix}
        \right\vert \\
        \tensor{\nu}{^i}
        & = \left(-1\right)^{i+1}
        \left\vert
        \begin{matrix}
        -\abs{k}\tan_k{\tensor{\theta}{^1}}\tensor{p}{^1} & k\cot_k{\tensor{\theta}{^1}}\tensor{p}{^2} & 0 & \dots \\
        -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^1} & -\abs{k}\tan_k{\tensor{\theta}{^2}}\tensor{p}{^2} & k\cot_k{\tensor{\theta}{^2}}\tensor{p}{^3} & \dots \\
        \dots & \dots & \dots & \dots \\
        \end{matrix}
        \right\vert & \text{(removed $\tensor{p}{^i}$ terms)} \\
        \tensor{\nu}{^1}
        & = k^n \prod{\cot_k{\tensor{\theta}{^i}}\tensor{p}{^{i+1}}} \\
        \tensor{\nu}{^2}
        & = k^n \sign{k} \tan_k{\tensor{\theta}{^1}}\tensor{p}{^2}\prod{\cot_k{\tensor{\theta}{^i}}\tensor{p}{^{i+1}}} \\
        \dots &= \dots
    \end{align*}
\end{proof}
\begin{lemma}\label{Model:ShapeOperator}
    \begin{equation*}
        S_{P}\left(v\right)
        = \tensor{v}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:ShapeOperator}]
    \skipped

    \begin{align*}
        S_{P}\left(v\right)
                   & = \nabla_v \nu\left(\tensor{P}{}\right)     \\
        \nabla_v f & = v \cdot \frac{\partial f}{\partial x}     \\
        \frac{\partial\nu\left(\tensor{P}{}\right)}{\partial\tensor{\theta}{^i}}
                   & = \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                             \\
        \end{bmatrix}                \\
        S_{\tensor{P}{}}\left(v\right)
                   & = \tensor{v}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{align*}
    false (need to map the derivative to tangent vector space at $\tensor{\theta}{}$)
\end{proof}
\begin{lemma}\label{Model:SecondFundamental}
    \begin{equation*}
        \text{II}_{\tensor{P}{}}\left(v,w\right) = \tensor{v}{^i}\tensor{w}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{equation*}
\end{lemma}
\begin{proof}[\proofof{Model:SecondFundamental}]
    \skipped

    \begin{align*}
        \text{II}_{\tensor{P}{}}\left(v,w\right)
         & = S_{\tensor{P}{}\left(v\right)} \cdot w                  \\
         & = v \cdot w \cdot \frac{\partial f}{\partial x}           \\
         & = \tensor{v}{^i}\tensor{w}{^i} \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{v}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                        \\
        \end{bmatrix}
    \end{align*}
    false (effected)
\end{proof}
\begin{lemma}\label{Model:PrincipalCurvature}

\end{lemma}
\begin{proof}[\proofof{Model:PrincipalCurvature}]
    \skipped

    Eigenvector and Eigenvalue can be solved from the following matrix
    \begin{align*}
        \begin{bmatrix}
            -\sign{k}\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^1} \\
            -\tan_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}         \\
            -\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^j}\sign{k} \\
            0                                                             \\
        \end{bmatrix}
    \end{align*}
    which is an upper triangular matrix.

    Eigenvalue is then the diagonal element
    \begin{align*}
        -\sign{k}\tan_k\left(\tensor{\theta}{^1}\right)\tensor{p}{^1} \\
        -\cot_k\left(\tensor{\theta}{^i}\right)\tensor{p}{^i}\sign{k} \\
        -\cot_k\left(\tensor{\theta}{^n}\right)
    \end{align*}
    false (effected)

    product of any 2 should be the value $\kappa$.
\end{proof}
\subsubsection{Curvature (Method II)}
This method may be easier to be done
(despite the fact that it never finished).
But it raises problems when trying to generalize e.g. dealing with extrinsic curvature
(which may be introduced in Model III
if not to mess with other basis geometries).
\begin{lemma}\label{Model:ChristoffelSymbol}

\end{lemma}
\begin{lemma}\label{Model:RiemannCurvatureTensor}

\end{lemma}
\subsubsection{Curvature (Conclusion)}
\begin{lemma}\label{Model:SectionalCurvature}

\end{lemma}
\proto{Trace back on why $k$ is used all along instead of $\kappa$}
\paragraph{CurvatureParameter}
It can be seen that $\operatorname{sec}\left(p\right) = \kappa = \sign{\left(k\right)}k^2$.
Hence, when provided $\kappa$, $k$ can be determined and used to evaluate the model.
\section{The Model}
\proto{Merge all element back to the model}
\stepcounter{Counter}
\begin{ModelGroupElement}
    For any parameter $\kappa, n$,
    \begin{align*}
        \elements                   & \defeq M                                    \\
        \groupoperation{\elements}  & \defeq \cdot                                \\
        \charts{\elements}          & \defeq \tensor{X}{}\mapsto\tensor{\theta}{} \\
        \innerprod{\elements}       & \defeq g                                    \\
        \points{\elements}          & \equiv R                                    \\
        \transformations{\elements} & \equiv M                                    \\
    \end{align*}
    for injective smooth function $K:\kappa \mapsto k = \sign{\kappa}\sqrt{\abs{\kappa}}\in\real\to\real$.
\end{ModelGroupElement}
\begin{ModelGroupAssertion}
\end{ModelGroupAssertion}
\begin{ModelCurvatureAssertion}
\end{ModelCurvatureAssertion}
\section{Future plan}
\subsection{Model II}
It is very simple to be able to model composite geometries e.g. $S^2 \times E$ by tensor product of the existing model. But to be able to merge them as smooth model may be challenging since not all combination of basis curvature have their own intrinsic geometry. So it may be to find independent variable for each basis or to introduce extrinsic curvature (Model A).
\subsection{Model B}
It is known that $E$ emerged at $n\ge1$ while $S$ and $H$ emerged at $n\ge2$ and there's more complex pure geometries than these that emerged in higher dimension. It is interesting and challenging to explore such geometries and prove whether the curvature still works as indicator in such geometries or are there any patterns for their symmetries.
\subsection{Model A}
This model is based on curvature and mostly just 3 basis geometries and extrinsic curvature which seems to be interesting despite some critical result in some combination e.g. $S^1 \times S^1$ vs $S^2$. It can be even more challenging to have variable curvature with respect to other intrinsic position.
\section*{}
\printbibliography
\begin{figure}
    \centering
    \insertstandalone{figures}{2dplot}
    \caption{Generalized trigonometric functions as function of $k$}\label{TrigonometryPlotted}
    \figurenote{This graph shows the value of generalized trigonometric functions as solid line and trigonometric and hyperbolic functions in the unused domain as dashed line.}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{sine}
    \caption{Generalized sine function}\label{TrigonometrySinePlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{cosine}
    \caption{Generalized cosine function}\label{TrigonometryCosinePlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{tangent}
    \caption{Generalized tangent function}\label{TrigonometryTangentPlotted}
\end{figure}
\begin{figure}
    \centering
    \insertstandalone{figures}{pointmapping}
    \caption{Matrices-Vectors-Parameters Mapping Diagram}\label{PointMappingDiagram}
\end{figure}
\end{document}